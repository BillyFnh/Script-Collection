# General
[instrumentation.lastSent]
search = index=_telemetry source=telemetry sourcetype=splunk_telemetry_log status=success | fillnull value=anonymous visibility | eval anonymous_send_time = if(visibility LIKE "%anonymous%", _time, null) | eval license_send_time = if(visibility LIKE "%license%", _time, null) | eval support_send_time = if(visibility LIKE "%support%", _time, null) | stats latest(anonymous_send_time) as latest_anonymous_send_time latest(license_send_time) as latest_license_send_time latest(support_send_time) as latest_support_send_time

[instrumentation.reportingErrorCount]
search = index=_telemetry source=telemetry sourcetype=splunk_telemetry_log status=failed | fillnull value=anonymous visibility | stats count(eval(visibility LIKE "%anonymous%")) as anonymous_errors count(eval(visibility LIKE "%license%")) as license_errors count(eval(visibility LIKE "%support%")) as support_errors

# Anonymous
# For splunk core <= 7.0.x and splunk_instrumentation <= 3.0.x, anonymous usage data is indexed in _telemtry
# For later versions, data is indexed in _introspection
[instrumentation.anonymized.eventsByTime]
search = (index=_introspection OR index=_telemetry) sourcetype=splunk_telemetry source="http-stream" visibility=*anonymous* | append [| savedsearch instrumentation.licenseUsage]

# Support
# For splunk core <= 7.0.x and splunk_instrumentation <= 3.0.x, support usage data is indexed in _telemtry
# For later versions, data is indexed in _introspection
[instrumentation.support.eventsByTime]
search = (index=_introspection OR index=_telemetry) sourcetype=splunk_telemetry source="http-stream" visibility=*support* | append [| savedsearch instrumentation.licenseUsage]

# Deployment
[instrumentation.deployment.clustering.indexer]
search = | makeresults annotate=true | append [localop | rest /services/cluster/config] | sort -mode | head 1 | eval data=if(mode=="master","{"host":""+splunk_server+"","timezone":""+strftime(now(),"%z")+"","multiSite":"+multisite+","summaryReplication":"+if(summary_replication=1,"true","false")+","enabled":true,"replicationFactor":"+tostring(replication_factor)+","siteReplicationFactor":"+coalesce(replace(replace(site_replication_factor, "origin", ""origin""), "total", ""total""), "null")+","siteSearchFactor":"+coalesce(replace(replace(site_search_factor, "origin", ""origin""), "total", ""total""),"null")+","searchFactor":"+tostring(search_factor)+"}","{"host":""+splunk_server+"","timezone":""+strftime(now(),"%z")+"","enabled":false}") | eval _time=now() | eval date=strftime(_time, "%Y-%m-%d") | fields _time date data

search = index=_internal source=*metrics.log* TERM(group=tcpin_connections) (TERM(connectionType=cooked) OR TERM(connectionType=cookedSSL)) fwdType=* guid=* | rename sourceIp as forwarderHost | eval connectionType=case(fwdType=="uf" or fwdType=="lwf" or fwdType=="full", fwdType, 1==1,"Splunk fwder") | eval version=if(isnull(version),"pre 4.2",version) | bin _time span=1d | stats sum(kb) as kb, latest(connectionType) as connectionType, latest(arch) as arch, latest(os) as os, latest(version) as version latest(forwarderHost) as forwarderHost by guid _time | stats estdc(forwarderHost) as numHosts estdc(guid) as numInstances `instrumentation_distribution_values(kb)` by connectionType arch os version _time | eval data="{"hosts":"+tostring(numHosts)+","instances":"+tostring(numInstances)+","architecture":""+arch+"","os":""+os+"","splunkVersion":""+version+"","type":""+connectionType+"","bytes":{" + `instrumentation_distribution_strings("kb",1024,0)` + "}}" | eval date=strftime(_time, "%Y-%m-%d") | fields _time date data

[instrumentation.deployment.app]
search = | rest /services/apps/local | eval _time=now() | fields splunk_server title updated version disabled | eval data="{"host":""+splunk_server+"","name":""+title+"","version":""+coalesce(version, "")+"","enabled":"+if(disabled=0, "true", "false")+"}" | eval date=strftime(_time, "%Y-%m-%d") | fields data _time date

[instrumentation.deployment.node]
search = index=_introspection sourcetype=splunk_disk_objects component::Partitions | bin _time span=1d | stats latest(data.free) as partitionFree, latest(data.capacity) as partitionCapacity by host data.fs_type data.mount_point _time | eval partitionUtilized=round(1-partitionFree/partitionCapacity,2) | eval partitions="{"utilization":"+`instrumentation_number_format(partitionUtilized,1,2)`+","capacity":"+`instrumentation_number_format(partitionCapacity,1048576,0)`+","fileSystem":""+'data.fs_type' + ""}" | stats delim="," values(partitions) as partitions by host _time | rename _time as date | mvcombine partitions | rename date as _time | join type=left host _time [search index=_introspection sourcetype=splunk_resource_usage component::Hostwide | eval cpuUsage = 'data.cpu_system_pct' + 'data.cpu_user_pct' | rename data.mem_used as memUsage | bin _time span=1d | stats latest(data.cpu_count) as coreCount, latest(data.virtual_cpu_count) as virtualCoreCount, latest(data.mem) as memAvailable, latest(data.splunk_version) as splunkVersion, latest(data.cpu_arch) as cpuArch, latest(data.os_name) as osName, latest(data.os_name_ext) as osNameExt, latest(data.os_version) as osVersion, `instrumentation_distribution_values(cpuUsage)`, `instrumentation_distribution_values(memUsage)`, latest(data.instance_guid) as guid by host _time] | fillnull value="null" coreCount virtualCoreCount memAvailable | eval splunkVersion=coalesce("""+splunkVersion+""", "null"), cpuArch=coalesce("""+cpuArch+""", "null"), osName=coalesce("""+osName + """, "null"), osNameExt=coalesce("""+osNameExt+""", "null"), osVersion=coalesce("""+osVersion+""", "null"), guid=coalesce("""+guid+""", "null") | eval data = "{"guid":"+guid+","host":""+replace(host,""", "\"")+"","partitions": " + coalesce("[" + partitions + "]", "null") + ","cpu":{"architecture":"+cpuArch+","coreCount":" + tostring(coreCount)+ ","virtualCoreCount":"+tostring(virtualCoreCount)+","utilization":{" + `instrumentation_distribution_strings("cpuUsage",.01,2)` + "}},"memory":"+"{"capacity":"+ `instrumentation_number_format(memAvailable,1048576,0)`+","utilization":{" + `instrumentation_distribution_strings("memUsage",1/memAvailable,2)` + "}},"os":"+osName+","osExt":"+osNameExt + ","osVersion":"+osVersion+","splunkVersion":"+splunkVersion+"}" | eval date=strftime(_time, "%Y-%m-%d") | fields _time date data

[instrumentation.deployment.index]
search = | rest /services/data/indexes | join type=outer splunk_server title [| rest /services/data/indexes-extended] | append [| rest /services/data/indexes datatype=metric | join type=outer splunk_server title [| rest /services/data/indexes-extended datatype=metric]] | eval warm_bucket_size = if(isnotnull('bucket_dirs.home.warm_bucket_size'), 'bucket_dirs.home.warm_bucket_size', 'bucket_dirs.home.size') | eval cold_bucket_size_gb = tostring(round(coalesce('bucket_dirs.cold.bucket_size', 'bucket_dirs.cold.size', 0) / 1024, 2)) | eval warm_bucket_size_gb = tostring(round(coalesce(warm_bucket_size,0) / 1024, 2)) | eval hot_bucket_size = tostring(round(coalesce(total_size / 1024 - cold_bucket_size_gb - warm_bucket_size_gb, 0),2)) | eval hot_bucket_size_gb = tostring(round(coalesce(hot_bucket_size,0) / 1024, 2)) | eval thawed_bucket_size_gb = tostring(round(coalesce('bucket_dirs.thawed.bucket_size', 'bucket_dirs.thawed.size',0) / 1024, 2)) | eval warm_bucket_count = tostring(coalesce('bucket_dirs.home.warm_bucket_count', 0)) | eval hot_bucket_count = tostring(coalesce('bucket_dirs.home.hot_bucket_count',0)) | eval cold_bucket_count = tostring(coalesce('bucket_dirs.cold.bucket_count',0)) | eval thawed_bucket_count = tostring(coalesce('bucket_dirs.thawed.bucket_count',0)) | eval home_event_count = tostring(coalesce('bucket_dirs.home.event_count',0)) | eval cold_event_count = tostring(coalesce('bucket_dirs.cold.event_count',0)) | eval thawed_event_count = tostring(coalesce('bucket_dirs.thawed.event_count',0)) | eval home_bucket_capacity_gb = coalesce(if('homePath.maxDataSizeMB' == 0, ""unlimited"", round('homePath.maxDataSizeMB' / 1024, 2)), ""unlimited"") | eval cold_bucket_capacity_gb = coalesce(if('coldPath.maxDataSizeMB' == 0, ""unlimited"", round('coldPath.maxDataSizeMB' / 1024, 2)), ""unlimited"") | eval currentDBSizeGB = tostring(round(coalesce(currentDBSizeMB,0) / 1024, 2)) | eval maxTotalDataSizeGB = tostring(if(maxTotalDataSizeMB = 0, "unlimited", coalesce(round(maxTotalDataSizeMB / 1024, 2), "null"))) | eval minTime = tostring(coalesce(strptime(minTime,"%Y-%m-%dT%H:%M:%S%z"),"null")) | eval maxTime = tostring(coalesce(strptime(maxTime,"%Y-%m-%dT%H:%M:%S%z"),"null")) | eval total_bucket_count = tostring(if(isnotnull(total_bucket_count), total_bucket_count, 0)) | eval totalEventCount = tostring(coalesce(totalEventCount, 0)) | eval total_raw_size_gb = tostring(coalesce(round(total_raw_size / 1024, 2), "null")) | eval index_type = coalesce(datatype ,"event") | rename eai:acl.app as App | eval _time=now() | fields splunk_server, title,index_type, currentDBSizeGB, totalEventCount, total_bucket_count, total_raw_size_gb, minTime, maxTime, home_bucket_capacity_gb, cold_bucket_capacity_gb, hot_bucket_size_gb, warm_bucket_size_gb, cold_bucket_size_gb, thawed_bucket_size_gb, hot_bucket_count, warm_bucket_count, cold_bucket_count, thawed_bucket_count, home_event_count, cold_event_count, thawed_event_count, maxTotalDataSizeGB, maxHotBuckets, maxWarmDBCount App _time | eval data="{"host":""+splunk_server+"","name":""+title+"","type":""+index_type+"","app":""+App+"","total":{"currentDBSizeGB":"+currentDBSizeGB+","maxDataSizeGB":"+maxTotalDataSizeGB+","events":"+totalEventCount+","buckets":"+total_bucket_count+","rawSizeGB":"+total_raw_size_gb+","minTime":"+minTime+","maxTime":"+maxTime+"},"buckets":{"homeCapacityGB":"+home_bucket_capacity_gb+","homeEventCount":"+home_event_count+","coldCapacityGB":"+cold_bucket_capacity_gb+","hot":{"sizeGB":"+hot_bucket_size_gb+","count":"+hot_bucket_count+","max":"+maxHotBuckets+"},"warm":{"sizeGB":"+warm_bucket_size_gb+","count":"+warm_bucket_count+"},"cold":{"sizeGB":"+cold_bucket_size_gb+","count":"+cold_bucket_count+","events":"+cold_event_count+"},"thawed":{"sizeGB":"+thawed_bucket_size_gb+","count":"+thawed_bucket_count+","events":"+thawed_event_count+"}}}" | eval date=strftime(_time, "%Y-%m-%d") | fields data _time date

# Licensing
[instrumentation.licenseUsage]
# Why start with append? Otherwise, when running this saved search by itself, the results of the
# stats command are not reflected in the events. Instead, the events tab will only show the events
# as they existed in the pipeline before stats.
search = NOT() | append [search index=_telemetry type=RolloverSummary | eval date=strftime(_time-43200, "%Y-%m-%d") | eval licenseIDs=coalesce(replace(replace(replace(replace(licenseGuids,"[","[""),"]",""]"),",","","")," ", ""),"null"), subgroup_id=coalesce(subgroupId, "Production"), group_id=coalesce("""+licenseGroup+""", "null"), lmGuid=coalesce("""+guid+""", "null"), productType=coalesce("""+productType+""", "null"), type_id=if(substr(stack,1,16)="fixed-sourcetype", "fixed-sourcetype",stack) | stats max(_time) as lastTime latest(stacksz) as stack_quota, latest(poolsz) as pool_quota, sum(b) as consumption by pool stack host lmGuid licenseIDs type_id group_id subgroup_id productType date | rename stack as stack_id | eval pool="{"quota":" + pool_quota+","consumption":"+consumption+"}" | stats delim="," values(pool) as pools, max(lastTime) as lastTime max(stack_quota) as stack_quota sum(consumption) as stack_consumption by stack_id group_id subgroup_id type_id lmGuid host licenseIDs productType date | mvcombine pools | eval _raw="{"component":"licensing.stack","data":{"host":""+host+"","guid":"+lmGuid+","name":""+replace(stack_id,""", "\"")+"","type":"" + type_id + "","subgroup":"" + subgroup_id + "","product":"+productType+","quota":" + stack_quota+","consumption":"+stack_consumption+","pools":["+pools+"],"licenseIDs":"+licenseIDs+"}, "date":""+date+"","visibility":"anonymous,license"}", _time=lastTime]

[instrumentation.licensing.stack]
search = index=_telemetry source=*license_usage_summary.log* sourcetype=splunkd TERM(type=RolloverSummary) | eval date=strftime(_time, "%m-%d-%Y"), licenseIDs=coalesce(replace(replace(replace(replace(licenseGuids,"[","[""),"]",""]"),",","","")," ", ""),"null"), subgroup_id=coalesce(subgroupId, "Production"), group_id=coalesce("""+licenseGroup+""", "null"), lmGuid=coalesce("""+guid+""", "null"), productType=coalesce("""+productType+""", "null"), type_id=if(substr(stack,1,16)="fixed-sourcetype", "fixed-sourcetype",stack) | stats latest(stacksz) as stack_quota, latest(poolsz) as pool_quota, sum(b) as consumption by pool stack host lmGuid licenseIDs type_id group_id subgroup_id productType date | rename stack as stack_id | eval pool="{"quota":" + pool_quota+","consumption":"+consumption+"}" | stats delim="," values(pool) as pools, max(stack_quota) as stack_quota sum(consumption) as stack_consumption by stack_id group_id subgroup_id type_id lmGuid host licenseIDs productType date | mvcombine pools | eval data="{"host":""+host+"","guid":"+lmGuid+","name":""+replace(stack_id,""", "\"")+"","type":"" + type_id + "","subgroup":"" + subgroup_id + "","product":"+productType+","quota":" + stack_quota+","consumption":"+stack_consumption+","pools":["+pools+"],"licenseIDs":"+licenseIDs+"}" | eval _time=strptime(date, "%m-%d-%Y")-43200 | fields data _time



# Performance
[instrumentation.performance.indexing]
search = index=_internal TERM(group=thruput) TERM(name=index_thruput) source=*metrics.log* | bin _time span=30s | stats sum(kb) as kb sum(instantaneous_kbps) as instantaneous_kbps by host _time | bin _time span=1d | stats sum(kb) as totalKB `instrumentation_distribution_values(instantaneous_kbps)` by host _time | eval data="{"host":""+host+"","thruput":{"total":" + tostring(round(totalKB*1024)) + "," + `instrumentation_distribution_strings("instantaneous_kbps",1024,0)`+"}}" | eval date=strftime(_time, "%Y-%m-%d") | fields _time date data

[instrumentation.performance.search]
search = index=_audit sourcetype=audittrail TERM(action=search) TERM(info=completed) total_run_time=* | eval search_et=if(search_et="N/A", 0, search_et) | eval search_lt=if(search_lt="N/A", exec_time, min(exec_time,search_lt)) | eval timerange=search_lt-search_et | bin _time span=1d | stats latest(searched_buckets) as searched_buckets latest(total_slices) as total_slices latest(scan_count) as scan_count latest(timerange) as timerange latest(total_run_time) as runtime by search_id _time | stats `instrumentation_distribution_values(runtime)`, `instrumentation_distribution_values(searched_buckets)`, `instrumentation_distribution_values(total_slices)`, `instrumentation_distribution_values(scan_count)`, `instrumentation_distribution_values(timerange)` count as numSearches by _time | eval data="{"searches":"+tostring(numSearches)+","latency":{"+`instrumentation_distribution_strings("runtime",1,2)`+"},"buckets":{"+`instrumentation_distribution_strings("searched_buckets",1,2)`+"},"slices":{"+`instrumentation_distribution_strings("total_slices",1,2)`+"},"scanCount":{"+`instrumentation_distribution_strings("scan_count",1,2)`+"},"dayRange":{"+`instrumentation_distribution_strings("timerange",1/86400,2)`+"}}" | eval date=strftime(_time, "%Y-%m-%d") | fields _time date data



# Templates
[instrumentation.anonymous.firstEvent]
search = (index=_introspection OR index=_telemetry) sourcetype=splunk_telemetry source="http-stream" visibility=*anonymous* | append [savedsearch instrumentation.licenseUsage] | where date >= "$beginDate$" AND date <= "$endDate$" | head 1

[instrumentation.support.firstEvent]
search = (index=_introspection OR index=_telemetry) sourcetype=splunk_telemetry source="http-stream" visibility=*support* | append [savedsearch instrumentation.licenseUsage] | where date >= "$beginDate$" AND date <= "$endDate$" | head 1

[instrumentation.license.firstEvent]
search = | savedsearch instrumentation.licenseUsage | where date >= "$beginDate$" AND date <= "$endDate$" | head 1

[instrumentation.reporting]
search = index=_telemetry source=telemetry sourcetype=splunk_telemetry_log | fields _raw | spath | eval time_formatted = strftime(_time, "%Y-%m-%d %H:%M:%S") | search (status=success OR status=failed)

search = index=_telemetry source=telemetry sourcetype=splunk_telemetry_log status=failed visibility=*$visibility$*



# Usage
[instrumentation.usage.app.page]
search = index=_internal sourcetype=splunk_web_access uri_path="/*/app/*/*" NOT uri_path="/*/static/*" | eval uri_parts=split(uri_path, "/"),locale=mvindex(uri_parts,1), app=mvindex(uri_parts,3), page=mvindex(uri_parts,4) | bin _time span=1d | eventstats estdc(user) as appUsers count as appOccurrences by app _time | bin _time span=1d | stats latest(locale) as locale count as occurrences estdc(user) as users by app page appUsers appOccurrences _time | sort app -occurrences | streamstats count as pageRank by app _time | where pageRank<=10 | eval data="{"app":""+app+"","page":""+page+"","locale":""+locale+"","occurrences":" + tostring(occurrences) + ","users":" + tostring(users) + "}" | eval data=if(pageRank==1,data+";{"app":""+app+"","locale":""+locale+"","occurrences":" + tostring(appOccurrences) + ","users":" + tostring(appUsers) + "}", data) | stats values(data) as data by app appOccurrences appUsers _time | sort _time -appOccurrences | streamstats count as appRank by _time | where appRank<=25 | mvexpand data | makemv delim=";" data | mvexpand data | eval date=strftime(_time, "%Y-%m-%d") | fields _time date data

[instrumentation.usage.indexing.sourcetype]
search = index=_internal source=*metrics.log* TERM(group=per_sourcetype_thruput) | bin _time span=1d | stats sum(ev) as events, sum(kb) as size, estdc(host) as hosts by series _time | eval data="{"name":""+replace(series,""", "\"") + "","events":"+tostring(events)+","bytes":"+tostring(round(size*1024))+","hosts":"+tostring(hosts)+"}" | eval date=strftime(_time, "%Y-%m-%d") | fields _time date data

[instrumentation.usage.search.concurrent]
search = index=_introspection sourcetype=splunk_resource_usage component::PerProcess data.search_props.sid::* | bin _time span=10s | stats estdc(data.search_props.sid) AS concurrent_searches by _time host | bin _time span=1d | stats `instrumentation_distribution_values(concurrent_searches)` by host _time | eval data="{"host":""+host+"","searches":{" + `instrumentation_distribution_strings("concurrent_searches",1,0)` +"}}" | eval date=strftime(_time, "%Y-%m-%d") | fields _time date data

[instrumentation.usage.search.type]
search = index=_introspection sourcetype=splunk_resource_usage component::PerProcess data.search_props.sid::* | rename data.search_props.type as searchType | bin _time span=1d | stats estdc(data.search_props.sid) AS search_count by searchType _time | eval data="""+searchType+"":"+tostring(search_count) | stats delim="," values(data) as data by _time | rename _time as date | mvcombine data | eval data="{"+data+"}" | rename date as _time | eval date=strftime(_time, "%Y-%m-%d") | fields _time date data

[instrumentation.usage.users.active]
search = index=_audit sourcetype=audittrail TERM(action=search) user!="splunk-system-user" user!="n/a" | bin _time span=1d | stats estdc(user) as active by _time | eval data="{"active":"+tostring(active)+"}" | eval date=strftime(_time, "%Y-%m-%d") | fields _time date data


#Topology
[instrumentation.topology.deployment.clustering.member]
search = | localop | rest /services/cluster/master/peers | eval data="{"master":""+splunk_server+"","member":{"host":""+label+"","guid":""+title+"","status":""+status+""},"site":""+site+""}" | where isnotnull(data) | eval _time=now() | eval date=strftime(_time, "%Y-%m-%d") | fields _time date data

[instrumentation.topology.deployment.clustering.searchhead]
search = | localop | rest /services/cluster/master/searchheads | where splunk_server!=label | eval data="{"master":""+splunk_server+"","searchhead":{"host":""+label+"","guid":""+title+"","status":""+status+""},"site":""+site+""}" | where isnotnull(data) | eval _time=now() | eval date=strftime(_time, "%Y-%m-%d") | fields _time date data

[instrumentation.topology.deployment.shclustering.member]
search = | localop | rest /services/shcluster/captain/members | eval data="{"site":""+site+"","captain":""+splunk_server+"","member":{"host":""+label+"","guid":""+title+"","status":""+status+""}}" | where isnotnull(data) | eval _time=now() | eval date=strftime(_time, "%Y-%m-%d") | fields _time date data

[instrumentation.topology.deployment.distsearch.peer]
search = | localop | rest /services/search/distributed/peers | eval data="{"host":""+splunk_server+"","peer":{"host":""+peerName+"","guid":""+guid+"","status":""+status+""}}" | where isnotnull(data) | eval _time=now() | eval date=strftime(_time, "%Y-%m-%d") | fields _time date data

[instrumentation.topology.deployment.licensing.slave]
search = | localop | rest /services/licenser/slaves | eval data="{"master":""+splunk_server+"","slave":{"host":""+label+"","guid":""+title+"","pool":""+active_pool_ids+""}}" | where isnotnull(data) | eval _time=now() | eval date=strftime(_time, "%Y-%m-%d") | fields _time date data

#Workload management
[instrumentation.usage.workloadManagement.enabled]
search = NOT() | append [rest splunk_server=local /services/workloads/status | eval support='general.isSupported', enabled='general.enabled', os_name='general.os_name', os_version='general.os_version'| fields support, enabled, os_name, os_version, splunk_server]

search = NOT() | append [rest splunk_server=local /services/workloads/pools | where NOT title="general" | eval data="""+title+"":{"cpu weight":""+cpu_weight+"", "memory weight":""+mem_weight+""}" | stats list(data) AS poolList, count AS poolTotal by splunk_server | eval poolCombined=mvjoin(poolList, ", "), Ingest_pools=if(poolTotal>0,1,0), Search_pools=if(poolTotal>0,poolTotal-1,0) | fields poolTotal, poolCombined, Ingest_pools, Search_pools, splunk_server]

search = NOT() | append [rest splunk_server=local /services/workloads/rules | eval data="""+title+"":{"order":""+order+"", "predicate":""+predicate+"", "workload pool":""+workload_pool+""}" | stats list(data) AS ruleList, count AS ruleTotal by splunk_server | eval ruleCombined=mvjoin(ruleList, ", ") | fields ruleTotal, ruleCombined, splunk_server]

[instrumentation.usage.workloadManagement.report]
action.outputtelemetry = 1
action.outputtelemetry.param.anonymous = 1
action.outputtelemetry.param.support = 0
action.outputtelemetry.param.license = 0
action.outputtelemetry.param.optinrequired = 3
action.outputtelemetry.param.component = usage.workloadManagement.report
action.outputtelemetry.param.input = data
action.outputtelemetry.param.type = aggregate
alert.suppress = 0
alert.track = 0
counttype = number of events
cron_schedule = 0 3 1-create-csv-with-header.py billyLocalsavedsearches.conf contents.txt master.conf master-section-collection result.csv searchLocalsavedsearches.conf single-section.conf splunk-conf-extraction splunk-conf-extraction20190906.py splunk-conf-extraction.py splunkEnterpriseSecuritySuiteDefaultsavedsearches.conf uniqueHeaders.txt 1-create-csv-with-header.py billyLocalsavedsearches.conf contents.txt master.conf master-section-collection result.csv searchLocalsavedsearches.conf single-section.conf splunk-conf-extraction splunk-conf-extraction20190906.py splunk-conf-extraction.py splunkEnterpriseSecuritySuiteDefaultsavedsearches.conf uniqueHeaders.txt 1
dispatch.earliest_time = -1w
dispatch.latest_time = now
display.general.type = statistics
display.page.search.tab = statistics
enableSched = 1
quantity = 0
relation = greater than
search = |rest splunk_server=local /services/server/info | appendcols [|rest splunk_server=local /servicesNS/nobody/splunk_instrumentation/telemetry | fields telemetrySalt]| eval telemetrySalt=if(isnull(telemetrySalt), "", telemetrySalt), hashHost=sha1(telemetrySalt+splunk_server), roleCombine=mvjoin(server_roles, ", ") | fields guid, hashHost, roleCombine, splunk_server | join type=left splunk_server [|savedsearch instrumentation.usage.workloadManagement.enabled] | join type=left splunk_server [|savedsearch instrumentation.usage.workloadManagement.pools] | join type=left splunk_server [|savedsearch instrumentation.usage.workloadManagement.rules] | fillnull value=0 | eval data="{"host": ""+hashHost+"", "guid": ""+guid+"", "wlm supported": ""+support+"", "os": ""+os_name+"", "osVersion": ""+os_version+"", "wlm enabled": ""+enabled+"", "server roles": ""+roleCombine+""", poolTotal=if(isnull(poolTotal),0, poolTotal), Ingest_pools=if(isnull(Ingest_pools),0, Ingest_pools), Search_pools=if(isnull(Search_pools),0, Search_pools), ruleTotal=if(isnull(ruleTotal),0, ruleTotal) | eval data=if(support==1, data+", "pools":{"total count":""+poolTotal+"", "ingest pool count":""+Ingest_pools+"", "search pool count":""+Search_pools+"""+if(poolTotal>0, ", "+poolCombined, "")+"}" + ", "rules":{"total count":""+ruleTotal+"""+if(ruleTotal>0, ", "+ruleCombined, "")+"}}", data+"}"), _time=now(), date=strftime(_time, "%Y-%m-%d")| fields _time date data

#Password policy management
[instrumentation.usage.passwordPolicy.config]
action.outputtelemetry = 1
action.outputtelemetry.param.anonymous = 1
action.outputtelemetry.param.support = 0
action.outputtelemetry.param.license = 0
action.outputtelemetry.param.optinrequired = 3
action.outputtelemetry.param.component = usage.passwordPolicy.config
action.outputtelemetry.param.input = data
action.outputtelemetry.param.type = aggregate
alert.suppress = 0
alert.track = 0
counttype = number of events
cron_schedule = 0 3 1-create-csv-with-header.py billyLocalsavedsearches.conf contents.txt master.conf master-section-collection result.csv searchLocalsavedsearches.conf single-section.conf splunk-conf-extraction splunk-conf-extraction20190906.py splunk-conf-extraction.py splunkEnterpriseSecuritySuiteDefaultsavedsearches.conf uniqueHeaders.txt 1-create-csv-with-header.py billyLocalsavedsearches.conf contents.txt master.conf master-section-collection result.csv searchLocalsavedsearches.conf single-section.conf splunk-conf-extraction splunk-conf-extraction20190906.py splunk-conf-extraction.py splunkEnterpriseSecuritySuiteDefaultsavedsearches.conf uniqueHeaders.txt 1
dispatch.earliest_time = -1w
dispatch.latest_time = now
display.general.type = statistics
display.page.search.tab = statistics
enableSched = 1
quantity = 0
relation = greater than
search = |rest splunk_server=local /services/admin/Splunk-auth/splunk_auth| join type=left splunk_server [|rest splunk_server=local /services/server/info | fields guid, splunk_server] | appendcols [|rest splunk_server=local /servicesNS/nobody/splunk_instrumentation/telemetry | fields telemetrySalt]| eval telemetrySalt=if(isnull(telemetrySalt), "", telemetrySalt), hashHost=sha1(telemetrySalt+splunk_server)| replace "1" with "true", "0" with "false" in enablePasswordHistory,expireUserAccounts, forceWeakPasswordChange, lockoutUsers, verboseLoginFailMsg | eval data="{"host": ""+hashHost+"","guid": ""+guid+"", "constant login time":""+constantLoginTime+"", "enable password history":""+enablePasswordHistory+"", "expiration alert in days":""+expireAlertDays+"", "days until password expires":""+expirePasswordDays+"", "enable password expiration":""+expireUserAccounts+"", "force existing users to change weak passwords":""+forceWeakPasswordChange+"", "failed login attempts":""+lockoutAttempts+"", "lockout duration in minutes":""+lockoutMins+"", "lockout threshold in minutes":""+lockoutThresholdMins+"", "enable lockout users":""+lockoutUsers+"", "minimum number of digits":""+minPasswordDigit+"", "minimum number of characters":""+minPasswordLength+"", "minimum number of lowercase letters":""+minPasswordLowercase+"", "minimum number of special characters":""+minPasswordSpecial+"", "minimum number of uppercase letters":""+minPasswordUppercase+"", "password history count":""+passwordHistoryCount+"", "enable verbose login fail message":""+verboseLoginFailMsg+""}",_time=now(), date=strftime(_time, "%Y-%m-%d") | fields data _time date

#Health monitoring
[instrumentation.usage.healthMonitor.report]
action.outputtelemetry = 1
action.outputtelemetry.param.anonymous = 1
action.outputtelemetry.param.support = 0
action.outputtelemetry.param.license = 0
action.outputtelemetry.param.optinrequired = 3
action.outputtelemetry.param.component = usage.healthMonitor.report
action.outputtelemetry.param.input = data
action.outputtelemetry.param.type = aggregate
alert.suppress = 0
alert.track = 0
counttype = number of events
cron_schedule = 0 3 1-create-csv-with-header.py billyLocalsavedsearches.conf contents.txt master.conf master-section-collection result.csv searchLocalsavedsearches.conf single-section.conf splunk-conf-extraction splunk-conf-extraction20190906.py splunk-conf-extraction.py splunkEnterpriseSecuritySuiteDefaultsavedsearches.conf uniqueHeaders.txt 1-create-csv-with-header.py billyLocalsavedsearches.conf contents.txt master.conf master-section-collection result.csv searchLocalsavedsearches.conf single-section.conf splunk-conf-extraction splunk-conf-extraction20190906.py splunk-conf-extraction.py splunkEnterpriseSecuritySuiteDefaultsavedsearches.conf uniqueHeaders.txt 1
dispatch.earliest_time = -1w
dispatch.latest_time = now
display.general.type = statistics
display.page.search.tab = statistics
enableSched = 1
quantity = 0
relation = greater than
search = |rest splunk_server=local /services/server/health-config | eval thresh="" | foreach indicator*red,indicator*yellow [eval thresh =if('<<FIELD>>'!="", thresh+""<<FIELD>>":" + '<<FIELD>>' + ",", thresh)] | eval thresh=rtrim(thresh, ","), enabled=if(disabled=='' or disabled==0 or isnull(disabled), 1,0) | eval feature="""+title+"":{"threshold": {"+thresh+"}, "enabled": ""+enabled+""}", distinct=if(like(title, "feature%"), "feature", "alert") | eval disable=coalesce('alert.disabled', disabled), action=coalesce('alert.actions','action.to','action.url', 'action.integration_url_override') | eval action=if(action=="" or isnull(action), "empty", action) | eval alert="""+title+"": {"disabled": ""+disable+"", "action/ action.to/ action.url/ action.integration_url_override": ""+action+""}" | stats list(alert) AS alertList, list(feature) AS feaList by distinct | eval alertCombined=mvjoin(alertList, ","), feaCombined=mvjoin(feaList, ",") | eval alertCombined=""alert":{"+alertCombined+"}" | eval feaCombined=if(distinct=="alert", null, feaCombined), alertCombined=if(distinct=="feature", null, alertCombined) | eval dataCombined=coalesce(alertCombined, feaCombined) | stats list(dataCombined) AS dataList| eval data=mvjoin(dataList, ",") | eval data="{"+data+"}",_time=now(), date=strftime(_time, "%Y-%m-%d") | fields data _time date

#Authentication methods
[instrumentation.usage.authMethod.config]
action.outputtelemetry = 1
action.outputtelemetry.param.anonymous = 1
action.outputtelemetry.param.support = 0
action.outputtelemetry.param.license = 0
action.outputtelemetry.param.optinrequired = 3
action.outputtelemetry.param.component = usage.authMethod.config
action.outputtelemetry.param.input = data
action.outputtelemetry.param.type = aggregate
alert.suppress = 0
alert.track = 0
counttype = number of events
cron_schedule = 0 3 1-create-csv-with-header.py billyLocalsavedsearches.conf contents.txt master.conf master-section-collection result.csv searchLocalsavedsearches.conf single-section.conf splunk-conf-extraction splunk-conf-extraction20190906.py splunk-conf-extraction.py splunkEnterpriseSecuritySuiteDefaultsavedsearches.conf uniqueHeaders.txt 1-create-csv-with-header.py billyLocalsavedsearches.conf contents.txt master.conf master-section-collection result.csv searchLocalsavedsearches.conf single-section.conf splunk-conf-extraction splunk-conf-extraction20190906.py splunk-conf-extraction.py splunkEnterpriseSecuritySuiteDefaultsavedsearches.conf uniqueHeaders.txt 1
dispatch.earliest_time = -1w
dispatch.latest_time = now
display.general.type = statistics
display.page.search.tab = statistics
enableSched = 1
quantity = 0
relation = greater than
search = |rest splunk_server=local /services/admin/auth-services| join type=left splunk_server [|rest splunk_server=local /services/server/info | fields guid, splunk_server] | appendcols [|rest splunk_server=local /servicesNS/nobody/splunk_instrumentation/telemetry | fields telemetrySalt]| eval telemetrySalt=if(isnull(telemetrySalt), "", telemetrySalt), hashHost=sha1(telemetrySalt+splunk_server)| eval data="{"host": ""+hashHost+"","guid": ""+guid+"", "authentication method": ""+active_authmodule+"","mfa type": " +""" + if(mfa_type=="", "none", mfa_type) +""}", _time=now(), date=strftime(_time, "%Y-%m-%d") | fields data _time date

#S2 configuration
[instrumentation.usage.smartStore.global]
search = |rest splunk_server=local /services/configs/conf-server | where title in ("cachemanager","diskUsage", "clustering") | eval data="""+title+"":",hotlist_recency_secs=if(isnull(hotlist_recency_secs), "none", hotlist_recency_secs), hotlist_bloom_filter_recency_hours=if(isnull(hotlist_bloom_filter_recency_hours), "none", hotlist_bloom_filter_recency_hours) | eval data=if(title="diskUsage", data+"{"minFreeSpace":""+minFreeSpace+""}", data), data=if(title="cachemanager", data+"{"eviction_padding":""+eviction_padding+"","max_cache_size":""+max_cache_size+"", "hotlist_recency_secs":""+hotlist_recency_secs+"", "hotlist_bloom_filter_recency_hours":""+hotlist_bloom_filter_recency_hours+""}", data), data=if(title="clustering", data+"{"mode":""+mode+"""+if(mode="master", ","search_factor":""+search_factor+"","multisite":""+multisite+"","site_replication_factor":""+site_replication_factor+"","site_search_factor":""+site_search_factor+""}", "}"), data) | stats list(data) AS dataList BY splunk_server | eval globalConfig=""global config":{" + mvjoin(dataList, ",") + "}" | fields globalConfig, splunk_server

[instrumentation.usage.smartStore.perIndex]
search = |rest splunk_server=local /services/configs/conf-indexes | appendcols [|rest splunk_server=local /servicesNS/nobody/splunk_instrumentation/telemetry | fields telemetrySalt]| eval title_dist=if(match(title, "^([^_].*?)s*"),"external","internal"), s2Enabled=if(isnotnull(remotePath),"SmartStore enabled", "non-SmartStore enabled"),hotlist_recency_secs=if(isnull(hotlist_recency_secs), "none", hotlist_recency_secs), hotlist_bloom_filter_recency_hours=if(isnull(hotlist_bloom_filter_recency_hours), "none", hotlist_bloom_filter_recency_hours) | makejson frozenTimePeriodInSecs, hotlist_recency_secs, hotlist_bloom_filter_recency_hours, maxHotSpanSecs, maxGlobalDataSizeMB, output="indexConfig" | eval telemetrySalt=if(isnull(telemetrySalt), "", telemetrySalt), hashTitle=sha1(telemetrySalt+title), title_combine=title_dist+"_"+hashTitle, indexConfig="""+title_combine+"":" + indexConfig | stats list(hashTitle) AS titleList,list(indexConfig) AS indexList BY s2Enabled, splunk_server | eval indexConfig=mvjoin(indexList, ","), titleCombined="""+s2Enabled+"":"" + mvjoin(titleList, ",") +""" | stats list(titleCombined) AS s2List, list(indexConfig) AS indexList BY splunk_server| eval s2Enabled=""list of indexes":{" + mvjoin(s2List, ",") + "}", indexConfig=""per index config":{" + mvjoin(indexList, ",") + "}" | fields s2Enabled, indexConfig, splunk_server

[instrumentation.usage.smartStore.capacity]
search = |rest splunk_server=local /services/server/status/partitions-space | makejson available, capacity, free, fs_type, output="cap" | eval cap="""+title+"": "+cap+"" | stats list(cap) AS capList BY splunk_server | eval capCombined=""total storage capacity":{" + mvjoin(capList, ", ") + "}" | fields capCombined, splunk_server

[instrumentation.usage.smartStore.config]
action.outputtelemetry = 1
action.outputtelemetry.param.anonymous = 1
action.outputtelemetry.param.support = 0
action.outputtelemetry.param.license = 0
action.outputtelemetry.param.optinrequired = 3
action.outputtelemetry.param.component = usage.smartStore.Config
action.outputtelemetry.param.input = data
action.outputtelemetry.param.type = aggregate
alert.suppress = 0
alert.track = 0
counttype = number of events
cron_schedule = 0 3 1-create-csv-with-header.py billyLocalsavedsearches.conf contents.txt master.conf master-section-collection result.csv searchLocalsavedsearches.conf single-section.conf splunk-conf-extraction splunk-conf-extraction20190906.py splunk-conf-extraction.py splunkEnterpriseSecuritySuiteDefaultsavedsearches.conf uniqueHeaders.txt 1-create-csv-with-header.py billyLocalsavedsearches.conf contents.txt master.conf master-section-collection result.csv searchLocalsavedsearches.conf single-section.conf splunk-conf-extraction splunk-conf-extraction20190906.py splunk-conf-extraction.py splunkEnterpriseSecuritySuiteDefaultsavedsearches.conf uniqueHeaders.txt 1
dispatch.earliest_time = -1w
dispatch.latest_time = now
display.general.type = statistics
display.page.search.tab = statistics
enableSched = 1
quantity = 0
relation = greater than
search = |savedsearch instrumentation.usage.smartStore.global | join type=left splunk_server [|savedsearch instrumentation.usage.smartStore.perIndex] | join type=left splunk_server [|savedsearch instrumentation.usage.smartStore.capacity] | eval data="{"+globalConfig+", "+capCombined+", "+indexConfig+", "+s2Enabled+"}",_time=now(), date=strftime(_time, "%Y-%m-%d") | fields data _time date

#Metrics
[instrumentation.usage.search.report_acceleration]
search = | localop | rest /servicesNS/-/-/admin/summarization | stats count as existing_report_accelerations, sum(summary.access_count) as access_count_of_existing_report_accelerations | makejson access_count_of_existing_report_accelerations(int) existing_report_accelerations(int) output="data" | eval _time=now(), date=strftime(_time, "%Y-%m-%d") | fields _time date data

#searchtelemetry
[instrumentation.usage.search.searchTelemetry]
search = index=_introspection sourcetype=search_telemetry | rename search_commands{}.name as name | stats count by name | makejson output=commands | fields commands | mvcombine delim="," commands | nomv commands | eval _time=now(), date=strftime(_time, "%Y-%m-%d") | eval data="{ "commands" : [".commands."]}" | fields _time date data

#Lookup Definitions
search = |rest splunk_server=local /services/admin/transforms-lookup getsize=true | eval name = 'eai:acl.app' + "." + title | rename "eai:acl.sharing" AS sharing | eval is_temporal = if(isnull(time_field),0,1) | table name type is_temporal size sharing | join type=left name [rest splunk_server=local /services/admin/kvstore-collectionstats | table data | mvexpand data | spath input=data | table ns size | rename ns as name] | makejson output=lookups | stats list(lookups) as lookups | eval data = "{ "lookups" : [" . mvjoin(lookups,",") . "]}", _time = now(), date=strftime(_time, "%Y-%m-%d") | fields data _time date

#Bundle Replication
[instrumentation.performance.bundleReplication]
search = index=_internal source=*/metrics.log TERM(group=bundles_uploads) | bin _time span=1d | stats count as bundles_uploads_count avg(peer_count) as avg_peer_count avg(average_baseline_bundle_bytes) as avg_baseline_bundle_bytes max(average_baseline_bundle_bytes) as max_baseline_bundle_bytes avg(average_delta_bundle_bytes) as avg_delta_bundle_bytes max(average_delta_bundle_bytes) as max_delta_bundle_bytes sum(total_count) as total_count sum(delta_count) as total_delta_count sum(success_count) as total_success_count sum(baseline_count) as total_baseline_count sum(already_present_count) as total_already_present_count sum(total_msec_spent) as total_msec_spent sum(delta_msec_spent) as total_delta_msec_spent sum(total_bytes) as total_bytes sum(delta_bytes) as total_delta_bytes by host _time | makejson output=data | eval date=strftime(_time, "%Y-%m-%d") | fields _time date data
[DMC Alert - Total License Usage Near Daily Quota]
alert.digest_mode = 1
alert.expires = 7d
counttype = number of events
alert.suppress = 1
alert.suppress.period = 4h
alert.track = 1
action.email.sendresults = 1
action.email.inline = 1
cron_schedule = 3,33 1-create-csv-with-header.py billyLocalsavedsearches.conf contents.txt master.conf master-section-collection result.csv searchLocalsavedsearches.conf single-section.conf splunk-conf-extraction splunk-conf-extraction20190906.py splunk-conf-extraction.py splunkEnterpriseSecuritySuiteDefaultsavedsearches.conf uniqueHeaders.txt 1-create-csv-with-header.py billyLocalsavedsearches.conf contents.txt master.conf master-section-collection result.csv searchLocalsavedsearches.conf single-section.conf splunk-conf-extraction splunk-conf-extraction20190906.py splunk-conf-extraction.py splunkEnterpriseSecuritySuiteDefaultsavedsearches.conf uniqueHeaders.txt 1-create-csv-with-header.py billyLocalsavedsearches.conf contents.txt master.conf master-section-collection result.csv searchLocalsavedsearches.conf single-section.conf splunk-conf-extraction splunk-conf-extraction20190906.py splunk-conf-extraction.py splunkEnterpriseSecuritySuiteDefaultsavedsearches.conf uniqueHeaders.txt 1-create-csv-with-header.py billyLocalsavedsearches.conf contents.txt master.conf master-section-collection result.csv searchLocalsavedsearches.conf single-section.conf splunk-conf-extraction splunk-conf-extraction20190906.py splunk-conf-extraction.py splunkEnterpriseSecuritySuiteDefaultsavedsearches.conf uniqueHeaders.txt
description = You have used 90% of your total daily license quota.
dispatch.ttl = 14400
disabled = 1
enableSched = 1
quantity = 0
relation = greater than
search = | rest splunk_server_group=dmc_group_license_master /services/licenser/pools | join type=outer stack_id splunk_server [rest splunk_server_group=dmc_group_license_master /services/licenser/groups | search is_active=1 | eval stack_id=stack_ids | fields splunk_server stack_id is_active] | search is_active=1 | fields splunk_server, stack_id, used_bytes | join type=outer stack_id splunk_server [rest splunk_server_group=dmc_group_license_master /services/licenser/stacks | eval stack_id=title | eval stack_quota=quota | fields splunk_server stack_id stack_quota] | stats sum(used_bytes) as used_bytes max(stack_quota) as stack_quota by splunk_server | eval usedGB=round(used_bytes/1024/1024/1024,3) | eval totalGB=round(stack_quota/1024/1024/1024,3) | eval percentage=round(usedGB / totalGB, 3)*100 | fields splunk_server, percentage, usedGB, totalGB | where percentage > 90 | rename splunk_server AS Instance, percentage AS "License quota used (%)", usedGB AS "License quota used (GB)", totalGB as "Total license quota (GB)"

disabled = 1
alert.suppress = 0
alert.track = 1
auto_summarize.dispatch.earliest_time = -1d@h
counttype = number of events
cron_schedule = */15 1-create-csv-with-header.py billyLocalsavedsearches.conf contents.txt master.conf master-section-collection result.csv searchLocalsavedsearches.conf single-section.conf splunk-conf-extraction splunk-conf-extraction20190906.py splunk-conf-extraction.py splunkEnterpriseSecuritySuiteDefaultsavedsearches.conf uniqueHeaders.txt 1-create-csv-with-header.py billyLocalsavedsearches.conf contents.txt master.conf master-section-collection result.csv searchLocalsavedsearches.conf single-section.conf splunk-conf-extraction splunk-conf-extraction20190906.py splunk-conf-extraction.py splunkEnterpriseSecuritySuiteDefaultsavedsearches.conf uniqueHeaders.txt 1-create-csv-with-header.py billyLocalsavedsearches.conf contents.txt master.conf master-section-collection result.csv searchLocalsavedsearches.conf single-section.conf splunk-conf-extraction splunk-conf-extraction20190906.py splunk-conf-extraction.py splunkEnterpriseSecuritySuiteDefaultsavedsearches.conf uniqueHeaders.txt 1-create-csv-with-header.py billyLocalsavedsearches.conf contents.txt master.conf master-section-collection result.csv searchLocalsavedsearches.conf single-section.conf splunk-conf-extraction splunk-conf-extraction20190906.py splunk-conf-extraction.py splunkEnterpriseSecuritySuiteDefaultsavedsearches.conf uniqueHeaders.txt
description = One or more forwarders are missing.
enableSched = 1
quantity = 0
relation = greater than
search = | inputlookup dmc_forwarder_assets| search status="missing" | rename hostname as Instance

alert.suppress = 0
alert.digest_mode = 1
alert.expires = 7d
counttype = number of events
alert.track = 1
action.email.sendresults = 1
action.email.inline = 1
cron_schedule = 3 0 1-create-csv-with-header.py billyLocalsavedsearches.conf contents.txt master.conf master-section-collection result.csv searchLocalsavedsearches.conf single-section.conf splunk-conf-extraction splunk-conf-extraction20190906.py splunk-conf-extraction.py splunkEnterpriseSecuritySuiteDefaultsavedsearches.conf uniqueHeaders.txt 1-create-csv-with-header.py billyLocalsavedsearches.conf contents.txt master.conf master-section-collection result.csv searchLocalsavedsearches.conf single-section.conf splunk-conf-extraction splunk-conf-extraction20190906.py splunk-conf-extraction.py splunkEnterpriseSecuritySuiteDefaultsavedsearches.conf uniqueHeaders.txt 1-create-csv-with-header.py billyLocalsavedsearches.conf contents.txt master.conf master-section-collection result.csv searchLocalsavedsearches.conf single-section.conf splunk-conf-extraction splunk-conf-extraction20190906.py splunk-conf-extraction.py splunkEnterpriseSecuritySuiteDefaultsavedsearches.conf uniqueHeaders.txt
description = You have licenses that expired or will expire within 2 weeks.
dispatch.ttl = 14400
disabled = 1
enableSched = 1
quantity = 0
relation = greater than
search = | rest splunk_server_group=dmc_group_license_master /services/licenser/licenses | join type=outer group_id splunk_server [ rest splunk_server_group=dmc_group_license_master /services/licenser/groups | where is_active = 1 | rename title AS group_id | fields is_active group_id splunk_server] | where is_active = 1 | eval days_left = floor((expiration_time - now()) / 86400) | where (status == "EXPIRED" OR days_left < 15) AND NOT (quota = 1048576 OR label == "Splunk Enterprise Reset Warnings" OR label == "Splunk Lite Reset Warnings") | eval expiration_status = case(days_left >= 14, days_left." days left", days_left < 14 AND days_left >= 0, "Expires soon: ".days_left." days left", days_left < 0, "Expired") | eval total_gb=round(quota/1024/1024/1024,3) | fields splunk_server label license_hash type group_id total_gb expiration_time expiration_status | convert ctime(expiration_time) | rename splunk_server AS Instance label AS "Label" license_hash AS "License Hash" type AS Type group_id AS Group total_gb AS Size expiration_time AS "Expires On" expiration_status AS Status

alert.digest_mode = 1
alert.expires = 7d
counttype = number of events
alert.suppress = 1
alert.suppress.period = 1h
alert.track = 1
action.email.sendresults = 1
action.email.inline = 1
cron_schedule = 3,13,23,33,43,53 1-create-csv-with-header.py billyLocalsavedsearches.conf contents.txt master.conf master-section-collection result.csv searchLocalsavedsearches.conf single-section.conf splunk-conf-extraction splunk-conf-extraction20190906.py splunk-conf-extraction.py splunkEnterpriseSecuritySuiteDefaultsavedsearches.conf uniqueHeaders.txt 1-create-csv-with-header.py billyLocalsavedsearches.conf contents.txt master.conf master-section-collection result.csv searchLocalsavedsearches.conf single-section.conf splunk-conf-extraction splunk-conf-extraction20190906.py splunk-conf-extraction.py splunkEnterpriseSecuritySuiteDefaultsavedsearches.conf uniqueHeaders.txt 1-create-csv-with-header.py billyLocalsavedsearches.conf contents.txt master.conf master-section-collection result.csv searchLocalsavedsearches.conf single-section.conf splunk-conf-extraction splunk-conf-extraction20190906.py splunk-conf-extraction.py splunkEnterpriseSecuritySuiteDefaultsavedsearches.conf uniqueHeaders.txt 1-create-csv-with-header.py billyLocalsavedsearches.conf contents.txt master.conf master-section-collection result.csv searchLocalsavedsearches.conf single-section.conf splunk-conf-extraction splunk-conf-extraction20190906.py splunk-conf-extraction.py splunkEnterpriseSecuritySuiteDefaultsavedsearches.conf uniqueHeaders.txt
description = One or more of your indexer queues is reporting a fill percentage, averaged over the last 15 minutes, of 90% or more.
disabled = 1
dispatch.ttl = 14400
enableSched = 1
quantity = 0
relation = greater than
search = | rest splunk_server_group=dmc_group_indexer /services/server/introspection/queues | search title=tcpin_queue OR title=parsingQueue OR title=aggQueue OR title=typingQueue OR title=indexQueue | eval fifteen_min_fill_perc = round(value_cntr3_size_bytes_lookback / max_size_bytes 1-create-csv-with-header.py billyLocalsavedsearches.conf contents.txt master.conf master-section-collection result.csv searchLocalsavedsearches.conf single-section.conf splunk-conf-extraction splunk-conf-extraction20190906.py splunk-conf-extraction.py splunkEnterpriseSecuritySuiteDefaultsavedsearches.conf uniqueHeaders.txt 100,2) | fields title fifteen_min_fill_perc splunk_server | where fifteen_min_fill_perc > 90 | rename splunk_server as Instance, title AS "Queue name", fifteen_min_fill_perc AS "Average queue fill percentage (last 15min)"

[DMC Alert - Abnormal State of Indexer Processor]
alert.digest_mode = 1
alert.expires = 7d
alert.suppress = 1
alert.suppress.period = 30m
alert.track = 1
action.email.sendresults = 1
action.email.inline = 1
counttype = number of events
cron_schedule = 3,8,13,18,23,28,33,38,43,48,53,58 1-create-csv-with-header.py billyLocalsavedsearches.conf contents.txt master.conf master-section-collection result.csv searchLocalsavedsearches.conf single-section.conf splunk-conf-extraction splunk-conf-extraction20190906.py splunk-conf-extraction.py splunkEnterpriseSecuritySuiteDefaultsavedsearches.conf uniqueHeaders.txt 1-create-csv-with-header.py billyLocalsavedsearches.conf contents.txt master.conf master-section-collection result.csv searchLocalsavedsearches.conf single-section.conf splunk-conf-extraction splunk-conf-extraction20190906.py splunk-conf-extraction.py splunkEnterpriseSecuritySuiteDefaultsavedsearches.conf uniqueHeaders.txt 1-create-csv-with-header.py billyLocalsavedsearches.conf contents.txt master.conf master-section-collection result.csv searchLocalsavedsearches.conf single-section.conf splunk-conf-extraction splunk-conf-extraction20190906.py splunk-conf-extraction.py splunkEnterpriseSecuritySuiteDefaultsavedsearches.conf uniqueHeaders.txt 1-create-csv-with-header.py billyLocalsavedsearches.conf contents.txt master.conf master-section-collection result.csv searchLocalsavedsearches.conf single-section.conf splunk-conf-extraction splunk-conf-extraction20190906.py splunk-conf-extraction.py splunkEnterpriseSecuritySuiteDefaultsavedsearches.conf uniqueHeaders.txt
description = One or more of your indexers is reporting an abnormal state.
disabled = 1
dispatch.ttl = 14400
enableSched = 1
quantity = 0
relation = greater than
search = | rest splunk_server_group=dmc_group_indexer /services/server/introspection/indexer | fields splunk_server, average_KBps, status, reason | where status != "normal" | eval average_KBps = round(average_KBps, 0) | eval status= if(status=="normal", status, status." - ".reason) | fields - reason | rename splunk_server as Instance, average_KBps as "Average KB/s (last 30s)", status as Status

[DMC Alert - Search Peer Not Responding]
alert.digest_mode = 1
alert.expires = 7d
alert.suppress = 1
alert.suppress.period = 30m
alert.track = 1
action.email.sendresults = 1
action.email.inline = 1
counttype = number of events
cron_schedule = 3,8,13,18,23,28,33,38,43,48,53,58 1-create-csv-with-header.py billyLocalsavedsearches.conf contents.txt master.conf master-section-collection result.csv searchLocalsavedsearches.conf single-section.conf splunk-conf-extraction splunk-conf-extraction20190906.py splunk-conf-extraction.py splunkEnterpriseSecuritySuiteDefaultsavedsearches.conf uniqueHeaders.txt 1-create-csv-with-header.py billyLocalsavedsearches.conf contents.txt master.conf master-section-collection result.csv searchLocalsavedsearches.conf single-section.conf splunk-conf-extraction splunk-conf-extraction20190906.py splunk-conf-extraction.py splunkEnterpriseSecuritySuiteDefaultsavedsearches.conf uniqueHeaders.txt 1-create-csv-with-header.py billyLocalsavedsearches.conf contents.txt master.conf master-section-collection result.csv searchLocalsavedsearches.conf single-section.conf splunk-conf-extraction splunk-conf-extraction20190906.py splunk-conf-extraction.py splunkEnterpriseSecuritySuiteDefaultsavedsearches.conf uniqueHeaders.txt 1-create-csv-with-header.py billyLocalsavedsearches.conf contents.txt master.conf master-section-collection result.csv searchLocalsavedsearches.conf single-section.conf splunk-conf-extraction splunk-conf-extraction20190906.py splunk-conf-extraction.py splunkEnterpriseSecuritySuiteDefaultsavedsearches.conf uniqueHeaders.txt
description = One or more of your search peers is currently down.
disabled = 1
dispatch.ttl = 14400
enableSched = 1
quantity = 0
relation = greater than
search = | rest splunk_server=local /services/search/distributed/peers/ | where status!="Up" AND disabled=0 | fields peerName, status | rename peerName as Instance, status as Status

[DMC Alert - Critical System Physical Memory Usage]
alert.digest_mode = 1
alert.expires = 7d
alert.suppress = 1
alert.suppress.period = 30m
alert.track = 1
action.email.sendresults = 1
action.email.inline = 1
counttype = number of events
cron_schedule = 3,8,13,18,23,28,33,38,43,48,53,58 1-create-csv-with-header.py billyLocalsavedsearches.conf contents.txt master.conf master-section-collection result.csv searchLocalsavedsearches.conf single-section.conf splunk-conf-extraction splunk-conf-extraction20190906.py splunk-conf-extraction.py splunkEnterpriseSecuritySuiteDefaultsavedsearches.conf uniqueHeaders.txt 1-create-csv-with-header.py billyLocalsavedsearches.conf contents.txt master.conf master-section-collection result.csv searchLocalsavedsearches.conf single-section.conf splunk-conf-extraction splunk-conf-extraction20190906.py splunk-conf-extraction.py splunkEnterpriseSecuritySuiteDefaultsavedsearches.conf uniqueHeaders.txt 1-create-csv-with-header.py billyLocalsavedsearches.conf contents.txt master.conf master-section-collection result.csv searchLocalsavedsearches.conf single-section.conf splunk-conf-extraction splunk-conf-extraction20190906.py splunk-conf-extraction.py splunkEnterpriseSecuritySuiteDefaultsavedsearches.conf uniqueHeaders.txt 1-create-csv-with-header.py billyLocalsavedsearches.conf contents.txt master.conf master-section-collection result.csv searchLocalsavedsearches.conf single-section.conf splunk-conf-extraction splunk-conf-extraction20190906.py splunk-conf-extraction.py splunkEnterpriseSecuritySuiteDefaultsavedsearches.conf uniqueHeaders.txt
description = One or more instances has exceeded 90% memory usage.
disabled = 1
dispatch.ttl = 14400
enableSched = 1
quantity = 0
relation = greater than
search = | rest splunk_server_group=dmc_group_* /services/server/status/resource-usage/hostwide | eval percentage=round(mem_used/mem,3)*100 | where percentage > 90 | fields splunk_server, percentage, mem_used, mem | rename splunk_server AS Instance, mem AS "Physical memory installed (MB)", percentage AS "Memory used (%)", mem_used AS "Memory used (MB)"

[DMC Alert - Near Critical Disk Usage]
alert.digest_mode = 1
alert.expires = 7d
counttype = number of events
alert.suppress = 1
alert.suppress.period = 4h
alert.track = 1
action.email.sendresults = 1
action.email.inline = 1
cron_schedule = 3,33 1-create-csv-with-header.py billyLocalsavedsearches.conf contents.txt master.conf master-section-collection result.csv searchLocalsavedsearches.conf single-section.conf splunk-conf-extraction splunk-conf-extraction20190906.py splunk-conf-extraction.py splunkEnterpriseSecuritySuiteDefaultsavedsearches.conf uniqueHeaders.txt 1-create-csv-with-header.py billyLocalsavedsearches.conf contents.txt master.conf master-section-collection result.csv searchLocalsavedsearches.conf single-section.conf splunk-conf-extraction splunk-conf-extraction20190906.py splunk-conf-extraction.py splunkEnterpriseSecuritySuiteDefaultsavedsearches.conf uniqueHeaders.txt 1-create-csv-with-header.py billyLocalsavedsearches.conf contents.txt master.conf master-section-collection result.csv searchLocalsavedsearches.conf single-section.conf splunk-conf-extraction splunk-conf-extraction20190906.py splunk-conf-extraction.py splunkEnterpriseSecuritySuiteDefaultsavedsearches.conf uniqueHeaders.txt 1-create-csv-with-header.py billyLocalsavedsearches.conf contents.txt master.conf master-section-collection result.csv searchLocalsavedsearches.conf single-section.conf splunk-conf-extraction splunk-conf-extraction20190906.py splunk-conf-extraction.py splunkEnterpriseSecuritySuiteDefaultsavedsearches.conf uniqueHeaders.txt
description = You have used 80% of your disk capacity.
disabled = 1
enableSched = 1
quantity = 0
relation = greater than
search = | rest splunk_server_group=dmc_group_* /services/server/status/partitions-space | eval free = if(isnotnull(available), available, free) | eval usage = capacity - free | eval pct_usage = floor(usage / capacity 1-create-csv-with-header.py billyLocalsavedsearches.conf contents.txt master.conf master-section-collection result.csv searchLocalsavedsearches.conf single-section.conf splunk-conf-extraction splunk-conf-extraction20190906.py splunk-conf-extraction.py splunkEnterpriseSecuritySuiteDefaultsavedsearches.conf uniqueHeaders.txt 100) | where pct_usage > 80 | stats first(fs_type) as fs_type first(capacity) AS capacity first(usage) AS usage first(pct_usage) AS pct_usage by splunk_server, mount_point | eval usage = round(usage / 1024, 2) | eval capacity = round(capacity / 1024, 2) | rename splunk_server AS Instance mount_point as "Mount Point", fs_type as "File System Type", usage as "Usage (GB)", capacity as "Capacity (GB)", pct_usage as "Usage (%)"

[DMC Asset - Build Standalone Asset Table]
action.populate_lookup = 1
action.populate_lookup.dest = dmc_assets
run_on_startup = 1
cron_schedule = */1 1-create-csv-with-header.py billyLocalsavedsearches.conf contents.txt master.conf master-section-collection result.csv searchLocalsavedsearches.conf single-section.conf splunk-conf-extraction splunk-conf-extraction20190906.py splunk-conf-extraction.py splunkEnterpriseSecuritySuiteDefaultsavedsearches.conf uniqueHeaders.txt 1-create-csv-with-header.py billyLocalsavedsearches.conf contents.txt master.conf master-section-collection result.csv searchLocalsavedsearches.conf single-section.conf splunk-conf-extraction splunk-conf-extraction20190906.py splunk-conf-extraction.py splunkEnterpriseSecuritySuiteDefaultsavedsearches.conf uniqueHeaders.txt 1-create-csv-with-header.py billyLocalsavedsearches.conf contents.txt master.conf master-section-collection result.csv searchLocalsavedsearches.conf single-section.conf splunk-conf-extraction splunk-conf-extraction20190906.py splunk-conf-extraction.py splunkEnterpriseSecuritySuiteDefaultsavedsearches.conf uniqueHeaders.txt 1-create-csv-with-header.py billyLocalsavedsearches.conf contents.txt master.conf master-section-collection result.csv searchLocalsavedsearches.conf single-section.conf splunk-conf-extraction splunk-conf-extraction20190906.py splunk-conf-extraction.py splunkEnterpriseSecuritySuiteDefaultsavedsearches.conf uniqueHeaders.txt
run_n_times = 1
disabled = 0
description = This search establishes an updated cache of metadata necessary for the localhost to be included in DMC dashboards. This search will be disabled when user go throught the setup process. and will be re-enabled when user resets to factory mode.
enableSched = 1
dispatchAs = user
search = | `dmc_get_local_instance_asset` | rename search_groups AS search_group | inputlookup append=true dmc_assets | stats last(*) AS 1-create-csv-with-header.py billyLocalsavedsearches.conf contents.txt master.conf master-section-collection result.csv searchLocalsavedsearches.conf single-section.conf splunk-conf-extraction splunk-conf-extraction20190906.py splunk-conf-extraction.py splunkEnterpriseSecuritySuiteDefaultsavedsearches.conf uniqueHeaders.txt by peerURI, search_group | fields peerURI serverName host machine search_group

[DMC Asset - Build Standalone Computed Groups Only]
action.populate_lookup = 1
action.populate_lookup.dest = dmc_assets
disabled = 0
dispatchAs = user
search = | `dmc_get_local_instance_asset` | rename search_groups AS search_group | fields peerURI serverName host machine search_group

[DMC Asset - Build Full]
action.populate_lookup = 1
action.populate_lookup.dest = dmc_assets
disabled = 0
dispatchAs = user
search = | rest splunk_server=local /services/search/distributed/peers | search status=Up disabled=0 | eval os = os_name | fields guid title peerName host host_fqdn server_roles search_groups cpu_arch os numberOfCores physicalMemoryMB version | rename title AS peerURI peerName AS serverName host_fqdn AS machine numberOfCores AS cpu_count physicalMemoryMB AS mem version AS splunk_version server_roles AS inherited_server_roles | where isnotnull(mvfind(search_groups,"dmc_group_")) | join type=outer peerURI [ | rest splunk_server=local /servicesNS/nobody/splunk_monitoring_console/configs/conf-splunk_monitoring_console_assets | fields title host host_fqdn | rename title AS peerURI host_fqdn AS machine] | mvexpand search_groups | append [ | `dmc_get_local_instance_asset_in_distributed_mode` ] | fields peerURI serverName host machine search_groups | rename search_groups AS search_group

[DMC Forwarder - Build Asset Table]
disabled = 1
enableSched = 1
cron_schedule = 3,18,33,48 1-create-csv-with-header.py billyLocalsavedsearches.conf contents.txt master.conf master-section-collection result.csv searchLocalsavedsearches.conf single-section.conf splunk-conf-extraction splunk-conf-extraction20190906.py splunk-conf-extraction.py splunkEnterpriseSecuritySuiteDefaultsavedsearches.conf uniqueHeaders.txt 1-create-csv-with-header.py billyLocalsavedsearches.conf contents.txt master.conf master-section-collection result.csv searchLocalsavedsearches.conf single-section.conf splunk-conf-extraction splunk-conf-extraction20190906.py splunk-conf-extraction.py splunkEnterpriseSecuritySuiteDefaultsavedsearches.conf uniqueHeaders.txt 1-create-csv-with-header.py billyLocalsavedsearches.conf contents.txt master.conf master-section-collection result.csv searchLocalsavedsearches.conf single-section.conf splunk-conf-extraction splunk-conf-extraction20190906.py splunk-conf-extraction.py splunkEnterpriseSecuritySuiteDefaultsavedsearches.conf uniqueHeaders.txt 1-create-csv-with-header.py billyLocalsavedsearches.conf contents.txt master.conf master-section-collection result.csv searchLocalsavedsearches.conf single-section.conf splunk-conf-extraction splunk-conf-extraction20190906.py splunk-conf-extraction.py splunkEnterpriseSecuritySuiteDefaultsavedsearches.conf uniqueHeaders.txt
run_on_startup = false
dispatch.earliest_time = -16m@m
dispatch.latest_time = -1m@m
dispatchAs = user
search = `dmc_build_forwarder_assets(1m)` | inputlookup append=true dmc_forwarder_assets | stats values(forwarder_type) as forwarder_type, max(version) as version, values(arch) as arch, values(os) as os, max(last_connected) as last_connected, values(new_sum_kb) as sum_kb, values(new_avg_tcp_kbps_sparkline) as avg_tcp_kbps_sparkline, values(new_avg_tcp_kbps) as avg_tcp_kbps, values(new_avg_tcp_eps) as avg_tcp_eps by guid, hostname | addinfo | eval status = if(isnull(sum_kb) or (sum_kb <= 0) or (last_connected < (info_max_time - 900)), "missing", "active") | eval sum_kb = round(sum_kb, 2) | eval avg_tcp_kbps = round(avg_tcp_kbps, 2) | eval avg_tcp_eps = round(avg_tcp_eps, 2) | fields guid, hostname, forwarder_type, version, arch, os, status, last_connected, sum_kb, avg_tcp_kbps_sparkline, avg_tcp_kbps, avg_tcp_eps | outputlookup dmc_forwarder_assets

# For license usage report dashboard
[DMC License Usage Data Cube]
dispatch.earliest_time = -31d
dispatch.latest_time = -0d
auto_summarize = 0
auto_summarize.dispatch.earliest_time = -1mon@d
auto_summarize.cron_schedule = 3,13,23,33,43,53 1-create-csv-with-header.py billyLocalsavedsearches.conf contents.txt master.conf master-section-collection result.csv searchLocalsavedsearches.conf single-section.conf splunk-conf-extraction splunk-conf-extraction20190906.py splunk-conf-extraction.py splunkEnterpriseSecuritySuiteDefaultsavedsearches.conf uniqueHeaders.txt 1-create-csv-with-header.py billyLocalsavedsearches.conf contents.txt master.conf master-section-collection result.csv searchLocalsavedsearches.conf single-section.conf splunk-conf-extraction splunk-conf-extraction20190906.py splunk-conf-extraction.py splunkEnterpriseSecuritySuiteDefaultsavedsearches.conf uniqueHeaders.txt 1-create-csv-with-header.py billyLocalsavedsearches.conf contents.txt master.conf master-section-collection result.csv searchLocalsavedsearches.conf single-section.conf splunk-conf-extraction splunk-conf-extraction20190906.py splunk-conf-extraction.py splunkEnterpriseSecuritySuiteDefaultsavedsearches.conf uniqueHeaders.txt 1-create-csv-with-header.py billyLocalsavedsearches.conf contents.txt master.conf master-section-collection result.csv searchLocalsavedsearches.conf single-section.conf splunk-conf-extraction splunk-conf-extraction20190906.py splunk-conf-extraction.py splunkEnterpriseSecuritySuiteDefaultsavedsearches.conf uniqueHeaders.txt
search = index=_internal source=*license_usage.log* type="Usage" | eval h=if(len(h)=0 OR isnull(h),"(SQUASHED)",h) | eval s=if(len(s)=0 OR isnull(s),"(SQUASHED)",s) | eval idx=if(len(idx)=0 OR isnull(idx),"(UNKNOWN)",idx) | bin _time span=1d | stats sum(b) as b by _time, host, pool, s, st, h, idx

# For DMC Heatmap
[default]
display.visualizations.custom.splunk_monitoring_console.heatmap.showTooltip = true
display.visualizations.custom.splunk_monitoring_console.heatmap.baseColor = #284774
display.visualizations.custom.splunk_monitoring_console.heatmap.showLegend = true
display.visualizations.custom.splunk_monitoring_console.heatmap.showXAxis = true
display.visualizations.custom.splunk_monitoring_console.heatmap.showYAxis = true
display.visualizations.custom.splunk_monitoring_console.heatmap.legendTitle = Instance count
display.visualizations.custom.splunk_monitoring_console.heatmap.xAxis = Time
display.visualizations.custom.splunk_monitoring_console.heatmap.yAxis =
# Version 7.3.1
search = error OR failed OR severe OR ( sourcetype=access_* ( 404 OR 500 OR 503 ) )
dispatch.earliest_time = -1d

[Errors in the last hour]
search = error OR failed OR severe OR ( sourcetype=access_* ( 404 OR 500 OR 503 ) )
dispatch.earliest_time = -1h

search = index=_internal source="*metrics.log" eps "group=per_source_thruput" NOT filetracker | eval events=eps*kb/kbps | timechart fixedrange=t span=1m limit=5 sum(events) by series
dispatch.earliest_time = -3h
displayview = report_builder_display

search = index=_internal " error " NOT debug source=*splunkd.log*
dispatch.earliest_time = -24h

search = | rest timeout=600 splunk_server=local /servicesNS/-/-/saved/searches add_orphan_field=yes count=0 | search orphan=1 disabled=0 is_scheduled=1 | eval status = if(disabled = 0, "enabled", "disabled") | fields title eai:acl.owner eai:acl.app eai:acl.sharing orphan status is_scheduled cron_schedule next_scheduled_time next_scheduled_time actions | rename title AS "search name" eai:acl.owner AS owner eai:acl.app AS app eai:acl.sharing AS sharing

# For license usage report dashboard
[License Usage Data Cube]
dispatch.earliest_time = -31d
dispatch.latest_time = -0d
auto_summarize = 0
auto_summarize.dispatch.earliest_time = -1mon@d
auto_summarize.cron_schedule = 3,13,23,33,43,53 1-create-csv-with-header.py billyLocalsavedsearches.conf contents.txt master.conf master-section-collection result.csv searchLocalsavedsearches.conf single-section.conf splunk-conf-extraction splunk-conf-extraction20190906.py splunk-conf-extraction.py splunkEnterpriseSecuritySuiteDefaultsavedsearches.conf uniqueHeaders.txt 1-create-csv-with-header.py billyLocalsavedsearches.conf contents.txt master.conf master-section-collection result.csv searchLocalsavedsearches.conf single-section.conf splunk-conf-extraction splunk-conf-extraction20190906.py splunk-conf-extraction.py splunkEnterpriseSecuritySuiteDefaultsavedsearches.conf uniqueHeaders.txt 1-create-csv-with-header.py billyLocalsavedsearches.conf contents.txt master.conf master-section-collection result.csv searchLocalsavedsearches.conf single-section.conf splunk-conf-extraction splunk-conf-extraction20190906.py splunk-conf-extraction.py splunkEnterpriseSecuritySuiteDefaultsavedsearches.conf uniqueHeaders.txt 1-create-csv-with-header.py billyLocalsavedsearches.conf contents.txt master.conf master-section-collection result.csv searchLocalsavedsearches.conf single-section.conf splunk-conf-extraction splunk-conf-extraction20190906.py splunk-conf-extraction.py splunkEnterpriseSecuritySuiteDefaultsavedsearches.conf uniqueHeaders.txt
search = index=_internal source=*license_usage.log* type="Usage" | eval h=if(len(h)=0 OR isnull(h),"(SQUASHED)",h) | eval s=if(len(s)=0 OR isnull(s),"(SQUASHED)",s) | eval idx=if(len(idx)=0 OR isnull(idx),"(UNKNOWN)",idx) | bin _time span=1d | stats sum(b) as b by _time, pool, s, st, h, idx

[403_by_clientip]
action.email.useNSSubject = 1
alert.track = 0
dispatch.earliest_time = 0
display.events.fields = ["host","source","sourcetype","method","action","file","process"]
display.general.type = statistics
display.page.search.tab = statistics
display.visualizations.show = 0
request.ui_dispatch_app = search
request.ui_dispatch_view = search
search = index="fundamental_one" status=403 | stats count as "Attempts" by clientip | sort - "Attempts"

alert.digest_mode = 0
alert.severity = 4
alert.suppress = 1
alert.suppress.fields = host
alert.suppress.period = 60s
alert.track = 1
counttype = number of events
cron_schedule = 1-create-csv-with-header.py billyLocalsavedsearches.conf contents.txt master.conf master-section-collection result.csv searchLocalsavedsearches.conf single-section.conf splunk-conf-extraction splunk-conf-extraction20190906.py splunk-conf-extraction.py splunkEnterpriseSecuritySuiteDefaultsavedsearches.conf uniqueHeaders.txt 1-create-csv-with-header.py billyLocalsavedsearches.conf contents.txt master.conf master-section-collection result.csv searchLocalsavedsearches.conf single-section.conf splunk-conf-extraction splunk-conf-extraction20190906.py splunk-conf-extraction.py splunkEnterpriseSecuritySuiteDefaultsavedsearches.conf uniqueHeaders.txt 1-create-csv-with-header.py billyLocalsavedsearches.conf contents.txt master.conf master-section-collection result.csv searchLocalsavedsearches.conf single-section.conf splunk-conf-extraction splunk-conf-extraction20190906.py splunk-conf-extraction.py splunkEnterpriseSecuritySuiteDefaultsavedsearches.conf uniqueHeaders.txt 1-create-csv-with-header.py billyLocalsavedsearches.conf contents.txt master.conf master-section-collection result.csv searchLocalsavedsearches.conf single-section.conf splunk-conf-extraction splunk-conf-extraction20190906.py splunk-conf-extraction.py splunkEnterpriseSecuritySuiteDefaultsavedsearches.conf uniqueHeaders.txt 1-create-csv-with-header.py billyLocalsavedsearches.conf contents.txt master.conf master-section-collection result.csv searchLocalsavedsearches.conf single-section.conf splunk-conf-extraction splunk-conf-extraction20190906.py splunk-conf-extraction.py splunkEnterpriseSecuritySuiteDefaultsavedsearches.conf uniqueHeaders.txt
dispatch.earliest_time = rt-1m
dispatch.latest_time = rt-0m
display.page.search.mode = fast
enableSched = 1
quantity = 0
relation = greater than
request.ui_dispatch_app = search
request.ui_dispatch_view = search
search = index="_audit" action="login attempt" info="failed" user="admin"
[Bucket Copy Trigger]
description = Triggers bucket copying
cron_schedule = 17 1-create-csv-with-header.py billyLocalsavedsearches.conf contents.txt master.conf master-section-collection result.csv searchLocalsavedsearches.conf single-section.conf splunk-conf-extraction splunk-conf-extraction20190906.py splunk-conf-extraction.py splunkEnterpriseSecuritySuiteDefaultsavedsearches.conf uniqueHeaders.txt 1-create-csv-with-header.py billyLocalsavedsearches.conf contents.txt master.conf master-section-collection result.csv searchLocalsavedsearches.conf single-section.conf splunk-conf-extraction splunk-conf-extraction20190906.py splunk-conf-extraction.py splunkEnterpriseSecuritySuiteDefaultsavedsearches.conf uniqueHeaders.txt 1-create-csv-with-header.py billyLocalsavedsearches.conf contents.txt master.conf master-section-collection result.csv searchLocalsavedsearches.conf single-section.conf splunk-conf-extraction splunk-conf-extraction20190906.py splunk-conf-extraction.py splunkEnterpriseSecuritySuiteDefaultsavedsearches.conf uniqueHeaders.txt 1-create-csv-with-header.py billyLocalsavedsearches.conf contents.txt master.conf master-section-collection result.csv searchLocalsavedsearches.conf single-section.conf splunk-conf-extraction splunk-conf-extraction20190906.py splunk-conf-extraction.py splunkEnterpriseSecuritySuiteDefaultsavedsearches.conf uniqueHeaders.txt
enableSched = 1
search = | archivebuckets

[Generate posix_identities lookup]
alert.track = 0
cron_schedule = */5 1-create-csv-with-header.py billyLocalsavedsearches.conf contents.txt master.conf master-section-collection result.csv searchLocalsavedsearches.conf single-section.conf splunk-conf-extraction splunk-conf-extraction20190906.py splunk-conf-extraction.py splunkEnterpriseSecuritySuiteDefaultsavedsearches.conf uniqueHeaders.txt 1-create-csv-with-header.py billyLocalsavedsearches.conf contents.txt master.conf master-section-collection result.csv searchLocalsavedsearches.conf single-section.conf splunk-conf-extraction splunk-conf-extraction20190906.py splunk-conf-extraction.py splunkEnterpriseSecuritySuiteDefaultsavedsearches.conf uniqueHeaders.txt 1-create-csv-with-header.py billyLocalsavedsearches.conf contents.txt master.conf master-section-collection result.csv searchLocalsavedsearches.conf single-section.conf splunk-conf-extraction splunk-conf-extraction20190906.py splunk-conf-extraction.py splunkEnterpriseSecuritySuiteDefaultsavedsearches.conf uniqueHeaders.txt 1-create-csv-with-header.py billyLocalsavedsearches.conf contents.txt master.conf master-section-collection result.csv searchLocalsavedsearches.conf single-section.conf splunk-conf-extraction splunk-conf-extraction20190906.py splunk-conf-extraction.py splunkEnterpriseSecuritySuiteDefaultsavedsearches.conf uniqueHeaders.txt
enableSched = 1
search = | inputlookup directory_posix_identities | eval source="directory" | append [|inputlookup local_posix_identities | eval source="local"] | append[|inputlookup learnt_posix_identities | rename _key as uid | search NOT uid=4294967295 | eval source="learnt"] | eventstats dc(user) as dc by uid | eval user=if(dc>1,uid,user) | dedup uid | table uid user | sort 0 +uid | outputlookup posix_identities

[Update auditd_host_inventory KVStore collection]
alert.track = 0
cron_schedule = 30 1-create-csv-with-header.py billyLocalsavedsearches.conf contents.txt master.conf master-section-collection result.csv searchLocalsavedsearches.conf single-section.conf splunk-conf-extraction splunk-conf-extraction20190906.py splunk-conf-extraction.py splunkEnterpriseSecuritySuiteDefaultsavedsearches.conf uniqueHeaders.txt 1-create-csv-with-header.py billyLocalsavedsearches.conf contents.txt master.conf master-section-collection result.csv searchLocalsavedsearches.conf single-section.conf splunk-conf-extraction splunk-conf-extraction20190906.py splunk-conf-extraction.py splunkEnterpriseSecuritySuiteDefaultsavedsearches.conf uniqueHeaders.txt 1-create-csv-with-header.py billyLocalsavedsearches.conf contents.txt master.conf master-section-collection result.csv searchLocalsavedsearches.conf single-section.conf splunk-conf-extraction splunk-conf-extraction20190906.py splunk-conf-extraction.py splunkEnterpriseSecuritySuiteDefaultsavedsearches.conf uniqueHeaders.txt 1-create-csv-with-header.py billyLocalsavedsearches.conf contents.txt master.conf master-section-collection result.csv searchLocalsavedsearches.conf single-section.conf splunk-conf-extraction splunk-conf-extraction20190906.py splunk-conf-extraction.py splunkEnterpriseSecuritySuiteDefaultsavedsearches.conf uniqueHeaders.txt
dispatch.earliest_time = -1h@h
dispatch.latest_time = @h
enableSched = 1
schedule_window = 15
search = [|inputlookup auditd_indices] [|inputlookup auditd_sourcetypes] type="DAEMON_START" | dedup host | rex field=kernel "^(?<kernel_version>[0-9-.]+).(?<distribution_release>[^.]+).?(?<architecture>.*)" | eval architecture =if(architecture=="","unknown",architecture) | table host kernel_version distribution_release architecture | rename host as _key | lookup auditd_host_inventory _key OUTPUT last_boot | outputlookup append=true auditd_host_inventory

[Update learnt_posix_identities KVStore collection]
alert.track = 0
cron_schedule = 30 1-create-csv-with-header.py billyLocalsavedsearches.conf contents.txt master.conf master-section-collection result.csv searchLocalsavedsearches.conf single-section.conf splunk-conf-extraction splunk-conf-extraction20190906.py splunk-conf-extraction.py splunkEnterpriseSecuritySuiteDefaultsavedsearches.conf uniqueHeaders.txt 1-create-csv-with-header.py billyLocalsavedsearches.conf contents.txt master.conf master-section-collection result.csv searchLocalsavedsearches.conf single-section.conf splunk-conf-extraction splunk-conf-extraction20190906.py splunk-conf-extraction.py splunkEnterpriseSecuritySuiteDefaultsavedsearches.conf uniqueHeaders.txt 1-create-csv-with-header.py billyLocalsavedsearches.conf contents.txt master.conf master-section-collection result.csv searchLocalsavedsearches.conf single-section.conf splunk-conf-extraction splunk-conf-extraction20190906.py splunk-conf-extraction.py splunkEnterpriseSecuritySuiteDefaultsavedsearches.conf uniqueHeaders.txt 1-create-csv-with-header.py billyLocalsavedsearches.conf contents.txt master.conf master-section-collection result.csv searchLocalsavedsearches.conf single-section.conf splunk-conf-extraction splunk-conf-extraction20190906.py splunk-conf-extraction.py splunkEnterpriseSecuritySuiteDefaultsavedsearches.conf uniqueHeaders.txt
dispatch.earliest_time = -1h@h
dispatch.latest_time = @h
enableSched = 1
schedule_window = 15
search = [|inputlookup auditd_indices] [|inputlookup auditd_sourcetypes] type="USER_START" acct=* NOT acct=root NOT auid=0 terminal=/dev/tty* OR NOT addr=? NOT auid=4294967295 | dedup auid | table auid acct | rename auid as _key | rename acct as user | outputlookup append=true learnt_posix_identities

[Update auditd_hosts lookup]
alert.track = 0
cron_schedule = 30 1-create-csv-with-header.py billyLocalsavedsearches.conf contents.txt master.conf master-section-collection result.csv searchLocalsavedsearches.conf single-section.conf splunk-conf-extraction splunk-conf-extraction20190906.py splunk-conf-extraction.py splunkEnterpriseSecuritySuiteDefaultsavedsearches.conf uniqueHeaders.txt 1-create-csv-with-header.py billyLocalsavedsearches.conf contents.txt master.conf master-section-collection result.csv searchLocalsavedsearches.conf single-section.conf splunk-conf-extraction splunk-conf-extraction20190906.py splunk-conf-extraction.py splunkEnterpriseSecuritySuiteDefaultsavedsearches.conf uniqueHeaders.txt 1-create-csv-with-header.py billyLocalsavedsearches.conf contents.txt master.conf master-section-collection result.csv searchLocalsavedsearches.conf single-section.conf splunk-conf-extraction splunk-conf-extraction20190906.py splunk-conf-extraction.py splunkEnterpriseSecuritySuiteDefaultsavedsearches.conf uniqueHeaders.txt 1-create-csv-with-header.py billyLocalsavedsearches.conf contents.txt master.conf master-section-collection result.csv searchLocalsavedsearches.conf single-section.conf splunk-conf-extraction splunk-conf-extraction20190906.py splunk-conf-extraction.py splunkEnterpriseSecuritySuiteDefaultsavedsearches.conf uniqueHeaders.txt
enableSched = 1
schedule_window = 30
search = | tstats values(host) as host WHERE [|inputlookup auditd_indices] [|inputlookup auditd_sourcetypes] | mvexpand host limit=0 | outputlookup auditd_hosts

[Update auditd_indices lookup]
alert.track = 0
cron_schedule = 0 */4 1-create-csv-with-header.py billyLocalsavedsearches.conf contents.txt master.conf master-section-collection result.csv searchLocalsavedsearches.conf single-section.conf splunk-conf-extraction splunk-conf-extraction20190906.py splunk-conf-extraction.py splunkEnterpriseSecuritySuiteDefaultsavedsearches.conf uniqueHeaders.txt 1-create-csv-with-header.py billyLocalsavedsearches.conf contents.txt master.conf master-section-collection result.csv searchLocalsavedsearches.conf single-section.conf splunk-conf-extraction splunk-conf-extraction20190906.py splunk-conf-extraction.py splunkEnterpriseSecuritySuiteDefaultsavedsearches.conf uniqueHeaders.txt 1-create-csv-with-header.py billyLocalsavedsearches.conf contents.txt master.conf master-section-collection result.csv searchLocalsavedsearches.conf single-section.conf splunk-conf-extraction splunk-conf-extraction20190906.py splunk-conf-extraction.py splunkEnterpriseSecuritySuiteDefaultsavedsearches.conf uniqueHeaders.txt
enableSched = 1
schedule_window = 60
search = | tstats values(sourcetype) as sourcetype where index=* [|inputlookup auditd_sourcetypes] by index | table index | outputlookup auditd_indices

[Update last_boot in auditd_host_inventory KVStore collection]
alert.track = 0
cron_schedule = */10 1-create-csv-with-header.py billyLocalsavedsearches.conf contents.txt master.conf master-section-collection result.csv searchLocalsavedsearches.conf single-section.conf splunk-conf-extraction splunk-conf-extraction20190906.py splunk-conf-extraction.py splunkEnterpriseSecuritySuiteDefaultsavedsearches.conf uniqueHeaders.txt 1-create-csv-with-header.py billyLocalsavedsearches.conf contents.txt master.conf master-section-collection result.csv searchLocalsavedsearches.conf single-section.conf splunk-conf-extraction splunk-conf-extraction20190906.py splunk-conf-extraction.py splunkEnterpriseSecuritySuiteDefaultsavedsearches.conf uniqueHeaders.txt 1-create-csv-with-header.py billyLocalsavedsearches.conf contents.txt master.conf master-section-collection result.csv searchLocalsavedsearches.conf single-section.conf splunk-conf-extraction splunk-conf-extraction20190906.py splunk-conf-extraction.py splunkEnterpriseSecuritySuiteDefaultsavedsearches.conf uniqueHeaders.txt 1-create-csv-with-header.py billyLocalsavedsearches.conf contents.txt master.conf master-section-collection result.csv searchLocalsavedsearches.conf single-section.conf splunk-conf-extraction splunk-conf-extraction20190906.py splunk-conf-extraction.py splunkEnterpriseSecuritySuiteDefaultsavedsearches.conf uniqueHeaders.txt
dispatch.earliest_time = -15m
dispatch.latest_time = now
enableSched = 1
schedule_window = 3
search = | pivot Auditd Auditd latest(_time) as time FILTER type in (SYSTEM_BOOT,DAEMON_START,SYSTEM_SHUTDOWN,DAEMON_END) SPLITROW host SPLITROW type | convert timeformat="%Y-%m-%dT%H:%M:%S.%3N%z" mktime(time) as last_boot | sort -last_boot | dedup host | eval last_boot=if(type=="SYSTEM_SHUTDOWN",0,round(last_boot)) | table host last_boot | rename host AS _key | lookup auditd_host_inventory _key OUTPUT kernel_version distribution_release architecture | outputlookup append=true auditd_host_inventory
[Cisco Security Suite - Overview - Global Security Events Map]
search = eventtype=cisco-security-events dest_ip!="255.255.255.255" dest_ip!="0.0.0.0" src_ip="*" | eval isLocalIP=`local-ip-list(src_ip)` | where isLocalIP!=1 AND isnotnull(threat_reason) AND threat_reason!="-" | stats count by src_ip | iplocation src_ip | geostats latfield=lat longfield=lon count by Country
dispatch.lookups = 1
dispatch.earliest_time = -60m
dispatch.latest_time = now
is_visible = false
display.general.enablePreview = true
display.general.timeRangePicker.show = true
auto_summarize = 1
auto_summarize.dispatch.earliest_time = -1d@h

[Cisco Security Suite - Overview - Security Event Stats by Host]
search = eventtype=cisco-security-events | chart count,sparkline(count) as "Trend" by host | sort -count
dispatch.lookups = 1
dispatch.earliest_time = -60m
dispatch.latest_time = now
is_visible = false
display.general.enablePreview = true
display.general.type = statistics
display.general.timeRangePicker.show = true
auto_summarize = 1
auto_summarize.dispatch.earliest_time = -1d@h

[Cisco Security Suite - Overview - Security Event Stats by Sourcetype]
search = eventtype=cisco-security-events | chart count,sparkline(count) as "Trend" by sourcetype | sort -count
dispatch.lookups = 1
dispatch.earliest_time = -60m
dispatch.latest_time = now
is_visible = false
display.general.enablePreview = true
display.general.type = statistics
display.general.timeRangePicker.show = true
auto_summarize = 1
auto_summarize.dispatch.earliest_time = -1d@h

search = eventtype=cisco-security-events dest_ip="*" dest_ip!="255.255.255.255" dest_ip!="0.0.0.0" | top dest_ip
dispatch.lookups = 1
dispatch.earliest_time = -60m
dispatch.latest_time = now
is_visible = false
display.general.enablePreview = true
display.general.type = visualizations
display.general.timeRangePicker.show = true
display.visualizations.show = true
display.visualizations.type = charting
display.visualizations.chartHeight = 600px
display.visualizations.charting.chart = pie
display.visualizations.charting.legend.placement = right

search = eventtype=cisco-security-events dest_ip!="255.255.255.255" dest_ip!="0.0.0.0" | eval port=coalesce(dest_port,src_port) | where isnotnull(port) | lookup networkservice "Port Number" as port OUTPUT "Service Name" AS service | eval service=if(isnull(service),"Port:"+tostring(port),service) | top service
dispatch.lookups = 1
dispatch.earliest_time = -60m
dispatch.latest_time = now
is_visible = false
display.general.enablePreview = true
display.general.type = visualizations
display.general.timeRangePicker.show = true
display.visualizations.show = true
display.visualizations.type = charting
display.visualizations.chartHeight = 600px
display.visualizations.charting.chart = pie
display.visualizations.charting.legend.placement = right

search = eventtype=cisco-security-events dest_ip!="255.255.255.255" dest_ip!="0.0.0.0" src_ip="*" | top src_ip
dispatch.lookups = 1
dispatch.earliest_time = -60m
dispatch.latest_time = now
is_visible = false
display.general.enablePreview = true
display.general.type = visualizations
display.general.timeRangePicker.show = true
display.visualizations.show = true
display.visualizations.type = charting
display.visualizations.chartHeight = 600px
display.visualizations.charting.chart = pie
display.visualizations.charting.legend.placement = right

search = eventtype=cisco-security-events dest_ip!="255.255.255.255" dest_ip!="0.0.0.0" | where isnotnull(threat_reason) AND threat_reason!="-" | eval product=if(isnull(product),"Cisco",product) | eval threat_reason=if(http_action="TCP_DENIED/407","authfail",threat_reason) | eval threat_reason="["+product+"]:"+threat_reason | top limit=10 threat_reason
dispatch.lookups = 1
dispatch.earliest_time = -60m
dispatch.latest_time = now
is_visible = false
display.general.enablePreview = true
display.general.type = visualizations
display.general.timeRangePicker.show = true
display.visualizations.show = true
display.visualizations.type = charting
display.visualizations.chartHeight = 600px
display.visualizations.charting.chart = pie
display.visualizations.charting.legend.placement = right

[Cisco ASA - Overview - Drop Reason]
search = eventtype=cisco-firewall action=block* | top cause
dispatch.lookups = 1
dispatch.earliest_time = -60m
dispatch.latest_time = now
is_visible = false
display.general.enablePreview = true
display.general.type = visualizations
display.general.timeRangePicker.show = true
display.visualizations.show = true
display.visualizations.type = charting
display.visualizations.chartHeight = 600px
display.visualizations.charting.chart = bar
display.visualizations.charting.chart.nullValueMode = zero
display.visualizations.charting.legend.placement = right

search = eventtype=cisco-firewall action="*" | timechart count by action
dispatch.lookups = 1
dispatch.earliest_time = -60m
dispatch.latest_time = now
is_visible = false
display.general.enablePreview = true
display.general.type = visualizations
display.general.timeRangePicker.show = true
display.visualizations.show = true
display.visualizations.type = charting
display.visualizations.chartHeight = 600px
display.visualizations.charting.chart = area
display.visualizations.charting.chart.stackMode = stacked
display.visualizations.charting.chart.nullValueMode = zero
display.visualizations.charting.legend.placement = right
display.visualizations.charting.axisTitleY.text = "# Events"
display.visualizations.charting.axisTitleY.visibility = visible
auto_summarize = 1
auto_summarize.dispatch.earliest_time = -1d@h

search = eventtype=cisco-firewall | top dest_ip
dispatch.lookups = 1
dispatch.earliest_time = -60m
dispatch.latest_time = now
is_visible = false
display.general.enablePreview = true
display.general.type = visualizations
display.general.timeRangePicker.show = true
display.visualizations.show = true
display.visualizations.type = charting
display.visualizations.chartHeight = 600px
display.visualizations.charting.chart = pie
display.visualizations.charting.legend.placement = right

search = eventtype=cisco-firewall service="*" | top service
dispatch.lookups = 1
dispatch.earliest_time = -60m
dispatch.latest_time = now
is_visible = false
display.general.enablePreview = true
display.general.type = visualizations
display.general.timeRangePicker.show = true
display.visualizations.show = true
display.visualizations.type = charting
display.visualizations.chartHeight = 600px
display.visualizations.charting.chart = pie
display.visualizations.charting.legend.placement = right

search = eventtype=cisco-firewall | top src_ip
dispatch.lookups = 1
dispatch.earliest_time = -60m
dispatch.latest_time = now
is_visible = false
display.general.enablePreview = true
display.general.type = visualizations
display.general.timeRangePicker.show = true
display.visualizations.show = true
display.visualizations.type = charting
display.visualizations.chartHeight = 600px
display.visualizations.charting.chart = pie
display.visualizations.charting.legend.placement = right

[Cisco ESA - Overview - Incoming Mail Over Time]
search = eventtype=cisco-esa | transaction keepevicted=true icid mid | search policy_direction="inbound" | timechart count by threat_reason
dispatch.lookups = 1
dispatch.earliest_time = -60m
dispatch.latest_time = now
is_visible = false
display.general.enablePreview = true
display.general.type = visualizations
display.general.timeRangePicker.show = true
display.visualizations.show = true
display.visualizations.type = charting
display.visualizations.chartHeight = 600px
display.visualizations.charting.chart = area
display.visualizations.charting.chart.nullValueMode = zero
display.visualizations.charting.chart.stackMode = stacked
display.visualizations.charting.legend.placement = right

[Cisco ESA - Overview - Messages Processed]
search = eventtype=cisco-esa | timechart dc(internal_message_id) as "Messages" by host
dispatch.lookups = 1
dispatch.earliest_time = -60m
dispatch.latest_time = now
is_visible = false
display.general.enablePreview = true
display.general.type = visualizations
display.general.timeRangePicker.show = true
display.visualizations.show = true
display.visualizations.type = charting
display.visualizations.chartHeight = 600px
display.visualizations.charting.chart = area
display.visualizations.charting.chart.nullValueMode = zero
display.visualizations.charting.legend.placement = right
auto_summarize = 1
auto_summarize.dispatch.earliest_time = -1d@h

[Cisco ESA - Overview - Outgoing Mail Over Time]
search = eventtype=cisco-esa | transaction keepevicted=true icid mid | search policy_direction="outbound" | timechart count by threat_reason
dispatch.lookups = 1
dispatch.earliest_time = -60m
dispatch.latest_time = now
is_visible = false
display.general.enablePreview = true
display.general.type = visualizations
display.general.timeRangePicker.show = true
display.visualizations.show = true
display.visualizations.type = charting
display.visualizations.chartHeight = 600px
display.visualizations.charting.chart = area
display.visualizations.charting.chart.nullValueMode = zero
display.visualizations.charting.chart.stackMode = stacked
display.visualizations.charting.legend.placement = right

search = eventtype=cisco-esa recipient="*" | top recipient
dispatch.lookups = 1
dispatch.earliest_time = -60m
dispatch.latest_time = now
is_visible = false
display.general.enablePreview = true
display.general.type = visualizations
display.general.timeRangePicker.show = true
display.visualizations.show = true
display.visualizations.type = charting
display.visualizations.chartHeight = 600px
display.visualizations.charting.chart = pie
display.visualizations.charting.legend.placement = right

search = eventtype=cisco-esa sender="*" | top sender
dispatch.lookups = 1
dispatch.earliest_time = -60m
dispatch.latest_time = now
is_visible = false
display.general.enablePreview = true
display.general.type = visualizations
display.general.timeRangePicker.show = true
display.visualizations.show = true
display.visualizations.type = charting
display.visualizations.chartHeight = 600px
display.visualizations.charting.chart = pie
display.visualizations.charting.legend.placement = right

search = eventtype=cisco-esa | timechart dc(dcid) as "Delivery Connections" by host
dispatch.lookups = 1
dispatch.earliest_time = -60m
dispatch.latest_time = now
is_visible = false
display.general.enablePreview = true
display.general.type = visualizations
display.general.timeRangePicker.show = true
display.visualizations.show = true
display.visualizations.type = charting
display.visualizations.chartHeight = 600px
display.visualizations.charting.chart = area
display.visualizations.charting.chart.nullValueMode = zero
display.visualizations.charting.legend.placement = right

search = eventtype=cisco-esa | timechart dc(icid) as "Incoming Connections" by host
dispatch.lookups = 1
dispatch.earliest_time = -60m
dispatch.latest_time = now
is_visible = false
display.general.enablePreview = true
display.general.type = visualizations
display.general.timeRangePicker.show = true
display.visualizations.show = true
display.visualizations.type = charting
display.visualizations.chartHeight = 600px
display.visualizations.charting.chart = area
display.visualizations.charting.chart.nullValueMode = zero
display.visualizations.charting.legend.placement = right
auto_summarize = 1
auto_summarize.dispatch.earliest_time = -1d@h

[Cisco ESA - Performance - Message Size]
search = eventtype=cisco-esa message_size="*" | timechart avg(message_size) as avg_size | eval avg_size = round((avg_size/(1024*1024)),2) | rename avg_size AS "Avg. Size"
dispatch.lookups = 1
dispatch.earliest_time = -60m
dispatch.latest_time = now
is_visible = false
display.general.enablePreview = true
display.general.type = visualizations
display.general.timeRangePicker.show = true
display.visualizations.show = true
display.visualizations.type = charting
display.visualizations.chartHeight = 600px
display.visualizations.charting.chart = area
display.visualizations.charting.chart.nullValueMode = zero
display.visualizations.charting.chart.stackMode = stacked
display.visualizations.charting.legend.placement = right
auto_summarize = 1
auto_summarize.dispatch.earliest_time = -1d@h

[Cisco ESA - Performance - Message Size Distribution]
search = eventtype=cisco-esa message_size="*" | eval msg_size=case(message_size <= 4096, "<4K", message_size > 4096 AND message_size <= 32768, "4K-32K", message_size > 32768 AND message_size <= 131072, "32K-128K", message_size > 131072 AND message_size <= 524288, "128K-512K", message_size > 524288 AND message_size <= 1048576, "512K-1M", message_size > 1048576 AND message_size <= 2097152, "1M-2M", message_size > 2097152 AND message_size <= 4194304, "2M-4M", message_size > 4194304 AND message_size <= 8388608, "4M-8M", message_size > 8388608, ">8M") | chart count by msg_size | sort - msg_size | rename msg_size AS "Message Size" count AS Count
dispatch.lookups = 1
dispatch.earliest_time = -60m
dispatch.latest_time = now
is_visible = false
display.general.enablePreview = true
display.general.type = visualizations
display.general.timeRangePicker.show = true
display.visualizations.show = true
display.visualizations.type = charting
display.visualizations.chartHeight = 600px
display.visualizations.charting.chart = bar
display.visualizations.charting.legend.placement = right
auto_summarize = 1
auto_summarize.dispatch.earliest_time = -1d@h

search = eventtype=cisco-esa | timechart dc(mid) as "Messages" by host
dispatch.lookups = 1
dispatch.earliest_time = -60m
dispatch.latest_time = now
is_visible = false
display.general.enablePreview = true
display.general.type = visualizations
display.general.timeRangePicker.show = true
display.visualizations.show = true
display.visualizations.type = charting
display.visualizations.chartHeight = 600px
display.visualizations.charting.chart = area
display.visualizations.charting.chart.nullValueMode = zero
display.visualizations.charting.legend.placement = right
auto_summarize = 1
auto_summarize.dispatch.earliest_time = -1d@h

search = eventtype=cisco_ips | top limit=10 attacker
dispatch.lookups = 1
dispatch.earliest_time = -60m
dispatch.latest_time = now
is_visible = false
search = eventtype=cisco_ips | iplocation src_ip | top limit=10 Country
dispatch.lookups = 1
dispatch.earliest_time = -60m
dispatch.latest_time = now
is_visible = false
[Cisco IPS - GC - Top HostID]
search = eventtype=cisco_ips | top limit=10 hostId
dispatch.lookups = 1
dispatch.earliest_time = -60m
dispatch.latest_time = now
is_visible = false
search = eventtype=cisco_ips | top limit=10 description | rename description as Signature
dispatch.lookups = 1
dispatch.earliest_time = -60m
dispatch.latest_time = now
is_visible = false
search = eventtype=cisco_ips | top limit=10 target
dispatch.lookups = 1
dispatch.earliest_time = -60m
dispatch.latest_time = now
is_visible = false
[Cisco IPS - GC - Global Threat Score Over Time]
search = eventtype=cisco_ips | timechart count by gc_score
dispatch.lookups = 1
dispatch.earliest_time = -60m
dispatch.latest_time = now
is_visible = false
auto_summarize = 1
auto_summarize.dispatch.earliest_time = -1d@h
[Cisco IPS - Overview - Average Risk Rating]
search = eventtype=cisco_ips | stats avg(risk_rating) as average | eval average=round(average, 1)
dispatch.lookups = 1
dispatch.earliest_time = -24h@h
dispatch.latest_time = now
is_visible = false
auto_summarize = 1
auto_summarize.dispatch.earliest_time = -1d@h
[Cisco IPS - Overview - Average Threat Rating]
search = eventtype=cisco_ips | stats avg(threat_rating) as average | eval average=round(average, 1)
dispatch.lookups = 1
dispatch.earliest_time = -24h@h
dispatch.latest_time = now
is_visible = false
auto_summarize = 1
auto_summarize.dispatch.earliest_time = -1d@h
[Cisco IPS - Overview - IPS Events By Severity]
search = eventtype=cisco_ips | transaction maxevents=2 maxspan=5m eventid | eval attack_count=if(isnotnull(summary_count),summary_count,1) | timechart sum(attack_count) by severity
dispatch.lookups = 1
dispatch.earliest_time = -24h@h
dispatch.latest_time = now
is_visible = false
[Cisco IPS - Overview - IPS Severity Distribution]
search = eventtype=cisco_ips | stats count by severity | sort - count
dispatch.lookups = 1
dispatch.earliest_time = -24h@h
dispatch.latest_time = now
is_visible = false
auto_summarize = 1
auto_summarize.dispatch.earliest_time = -1d@h
search = eventtype=cisco_ips | transaction maxevents=2 maxspan=5m eventid | eval attack_count=if(isnotnull(summary_count),summary_count,1) | stats sum(attack_count) as count,sparkline by attacker | sort - count
dispatch.lookups = 1
dispatch.earliest_time = -24h@h
dispatch.latest_time = now
is_visible = false
search = eventtype=cisco_ips | transaction maxevents=2 maxspan=5m eventid | eval attack_count=if(isnotnull(summary_count),summary_count,1) | stats sum(attack_count) as count,sparkline by severity,sig_type,description | sort - count | rename sig_type as Type
dispatch.lookups = 1
dispatch.earliest_time = -24h@h
dispatch.latest_time = now
is_visible = false
search = eventtype=cisco_ips | transaction maxevents=2 maxspan=5m eventid | eval attack_count=if(isnotnull(summary_count),summary_count,1) | stats sum(attack_count) as count,sparkline by target | sort - count
dispatch.lookups = 1
dispatch.earliest_time = -24h@h
dispatch.latest_time = now
is_visible = false
######################################################
#
# Lookup table builders
#
######################################################
action.email.inline = 1
alert.suppress = 0
alert.track = 0
cron_schedule = 0 1-create-csv-with-header.py billyLocalsavedsearches.conf contents.txt master.conf master-section-collection result.csv searchLocalsavedsearches.conf single-section.conf splunk-conf-extraction splunk-conf-extraction20190906.py splunk-conf-extraction.py splunkEnterpriseSecuritySuiteDefaultsavedsearches.conf uniqueHeaders.txt 1-create-csv-with-header.py billyLocalsavedsearches.conf contents.txt master.conf master-section-collection result.csv searchLocalsavedsearches.conf single-section.conf splunk-conf-extraction splunk-conf-extraction20190906.py splunk-conf-extraction.py splunkEnterpriseSecuritySuiteDefaultsavedsearches.conf uniqueHeaders.txt 1-create-csv-with-header.py billyLocalsavedsearches.conf contents.txt master.conf master-section-collection result.csv searchLocalsavedsearches.conf single-section.conf splunk-conf-extraction splunk-conf-extraction20190906.py splunk-conf-extraction.py splunkEnterpriseSecuritySuiteDefaultsavedsearches.conf uniqueHeaders.txt 1-create-csv-with-header.py billyLocalsavedsearches.conf contents.txt master.conf master-section-collection result.csv searchLocalsavedsearches.conf single-section.conf splunk-conf-extraction splunk-conf-extraction20190906.py splunk-conf-extraction.py splunkEnterpriseSecuritySuiteDefaultsavedsearches.conf uniqueHeaders.txt
description = Updates the ISE_Locations.csv lookup file
dispatch.earliest_time = -24h@h
dispatch.latest_time = now
enableSched = 1
run_on_startup = true
search = eventtype="css-ise" Location="*" | stats count by Location | inputlookup append=T ISE_Locations.csv | stats count by Location | table Location | outputlookup ISE_Locations.csv

[_lookup_create_interface_list]
auto_summarize.dispatch.earliest_time = -1d@h
cron_schedule = 0 0 1-create-csv-with-header.py billyLocalsavedsearches.conf contents.txt master.conf master-section-collection result.csv searchLocalsavedsearches.conf single-section.conf splunk-conf-extraction splunk-conf-extraction20190906.py splunk-conf-extraction.py splunkEnterpriseSecuritySuiteDefaultsavedsearches.conf uniqueHeaders.txt 1-create-csv-with-header.py billyLocalsavedsearches.conf contents.txt master.conf master-section-collection result.csv searchLocalsavedsearches.conf single-section.conf splunk-conf-extraction splunk-conf-extraction20190906.py splunk-conf-extraction.py splunkEnterpriseSecuritySuiteDefaultsavedsearches.conf uniqueHeaders.txt 1-create-csv-with-header.py billyLocalsavedsearches.conf contents.txt master.conf master-section-collection result.csv searchLocalsavedsearches.conf single-section.conf splunk-conf-extraction splunk-conf-extraction20190906.py splunk-conf-extraction.py splunkEnterpriseSecuritySuiteDefaultsavedsearches.conf uniqueHeaders.txt
dispatch.latest_time = now
enableSched = 1
search = sourcetype=cisco:esa* src_interface=* OR dest_interface=* | eval interface=trim(coalesce(src_interface,"") . "," . coalesce(dest_interface,""), ",") | makemv delim="," interface | stats count by interface | where len(interface)>0 | fields interface | rename interface as dest_interface | outputlookup interface_list

[Cisco WSA - Acceptable Use - Business vs. Other]
search = eventtype=css-wsa-squid | eval usage = if(usage="Business","Business Use","Other") | timechart count by usage
dispatch.lookups = 1
dispatch.earliest_time = -60m
dispatch.latest_time = now
is_visible = false
display.general.enablePreview = true
display.general.type = visualizations
display.general.timeRangePicker.show = true
display.visualizations.show = true
display.visualizations.type = charting
display.visualizations.chartHeight = 600px
display.visualizations.charting.chart = area
display.visualizations.charting.chart.nullValueMode = zero
display.visualizations.charting.legend.placement = none
display.visualizations.charting.axisTitleY.text = "# Transactions"
display.visualizations.charting.axisTitleY.visibility = visible
auto_summarize = 1
auto_summarize.dispatch.earliest_time = -1d@h

[Cisco WSA - Acceptable Use - Legal vs. Other]
search = eventtype=css-wsa-squid | eval usage = if(usage="Violation","Legal Liability","Other") | timechart count by usage
dispatch.lookups = 1
dispatch.earliest_time = -60m
dispatch.latest_time = now
is_visible = false
display.general.enablePreview = true
display.general.type = visualizations
display.general.timeRangePicker.show = true
display.visualizations.show = true
display.visualizations.type = charting
display.visualizations.chartHeight = 600px
display.visualizations.charting.chart = area
display.visualizations.charting.chart.nullValueMode = zero
display.visualizations.charting.legend.placement = none
display.visualizations.charting.axisTitleY.text = "# Transactions"
display.visualizations.charting.axisTitleY.visibility = visible
auto_summarize = 1
auto_summarize.dispatch.earliest_time = -1d@h

[Cisco WSA - Acceptable Use - Personal vs. Other]
search = eventtype=css-wsa-squid | eval usage = if(usage="Personal","Personal Use","Other") | timechart count by usage
dispatch.lookups = 1
dispatch.earliest_time = -60m
dispatch.latest_time = now
is_visible = false
display.general.enablePreview = true
display.general.type = visualizations
display.general.timeRangePicker.show = true
display.visualizations.show = true
display.visualizations.type = charting
display.visualizations.chartHeight = 600px
display.visualizations.charting.chart = area
display.visualizations.charting.chart.nullValueMode = zero
display.visualizations.charting.legend.placement = none
display.visualizations.charting.axisTitleY.text = "# Transactions"
display.visualizations.charting.axisTitleY.visibility = visible
auto_summarize = 1
auto_summarize.dispatch.earliest_time = -1d@h

search = eventtype=css-wsa-squid x_webcat_code_full="*" usage="Business" | top x_webcat_code_full | rename x_webcat_code_full as "Category"
dispatch.lookups = 1
dispatch.earliest_time = -60m
dispatch.latest_time = now
is_visible = false
display.general.enablePreview = true
display.general.type = visualizations
display.general.timeRangePicker.show = true
display.visualizations.show = true
display.visualizations.type = charting
display.visualizations.chartHeight = 600px
display.visualizations.charting.chart = bar
display.visualizations.charting.legend.placement = none

search = eventtype=css-wsa-squid src_ip="*" usage="Business" | top src_ip | rename src_ip as "Source IP"
dispatch.lookups = 1
dispatch.earliest_time = -60m
dispatch.latest_time = now
is_visible = false
display.general.enablePreview = true
display.general.type = visualizations
display.general.timeRangePicker.show = true
display.visualizations.show = true
display.visualizations.type = charting
display.visualizations.chartHeight = 600px
display.visualizations.charting.chart = bar
display.visualizations.charting.legend.placement = none

search = eventtype=css-wsa-squid cs_username="*" usage="Business" | top cs_username | rename cs_username as Username
dispatch.lookups = 1
dispatch.earliest_time = -60m
dispatch.latest_time = now
is_visible = false
display.general.enablePreview = true
display.general.type = visualizations
display.general.timeRangePicker.show = true
display.visualizations.show = true
display.visualizations.type = charting
display.visualizations.chartHeight = 600px
display.visualizations.charting.chart = bar
display.visualizations.charting.legend.placement = none

search = eventtype=css-wsa-squid cs_url_host="*" usage="Business" | top cs_url_host | rename cs_url_host as "Website"
dispatch.lookups = 1
dispatch.earliest_time = -60m
dispatch.latest_time = now
is_visible = false
display.general.enablePreview = true
display.general.type = visualizations
display.general.timeRangePicker.show = true
display.visualizations.show = true
display.visualizations.type = charting
display.visualizations.chartHeight = 600px
display.visualizations.charting.chart = bar
display.visualizations.charting.legend.placement = none

search = eventtype=css-wsa-squid x_webcat_code_full="*" usage="Violation" | top x_webcat_code_full | rename x_webcat_code_full as "Category"
dispatch.lookups = 1
dispatch.earliest_time = -60m
dispatch.latest_time = now
is_visible = false
display.general.enablePreview = true
display.general.type = visualizations
display.general.timeRangePicker.show = true
display.visualizations.show = true
display.visualizations.type = charting
display.visualizations.chartHeight = 600px
display.visualizations.charting.chart = bar
display.visualizations.charting.legend.placement = none

search = eventtype=css-wsa-squid src_ip="*" usage="Violation" | top src_ip | rename src_ip as "Source IP"
dispatch.lookups = 1
dispatch.earliest_time = -60m
dispatch.latest_time = now
is_visible = false
display.general.enablePreview = true
display.general.type = visualizations
display.general.timeRangePicker.show = true
display.visualizations.show = true
display.visualizations.type = charting
display.visualizations.chartHeight = 600px
display.visualizations.charting.chart = bar
display.visualizations.charting.legend.placement = none

search = eventtype=css-wsa-squid cs_username="*" usage="Violation" | top cs_username | rename cs_username as Username
dispatch.lookups = 1
dispatch.earliest_time = -60m
dispatch.latest_time = now
is_visible = false
display.general.enablePreview = true
display.general.type = visualizations
display.general.timeRangePicker.show = true
display.visualizations.show = true
display.visualizations.type = charting
display.visualizations.chartHeight = 600px
display.visualizations.charting.chart = bar
display.visualizations.charting.legend.placement = none

search = eventtype=css-wsa-squid cs_url_host="*" usage="Violation" | top cs_url_host | rename cs_url_host as "Website"
dispatch.lookups = 1
dispatch.earliest_time = -60m
dispatch.latest_time = now
is_visible = false
display.general.enablePreview = true
display.general.type = visualizations
display.general.timeRangePicker.show = true
display.visualizations.show = true
display.visualizations.type = charting
display.visualizations.chartHeight = 600px
display.visualizations.charting.chart = bar
display.visualizations.charting.legend.placement = none

search = eventtype=css-wsa-squid x_webcat_code_full="*" usage="Personal" | top x_webcat_code_full | rename x_webcat_code_full as "Category"
dispatch.lookups = 1
dispatch.earliest_time = -60m
dispatch.latest_time = now
is_visible = false
display.general.enablePreview = true
display.general.type = visualizations
display.general.timeRangePicker.show = true
display.visualizations.show = true
display.visualizations.type = charting
display.visualizations.chartHeight = 600px
display.visualizations.charting.chart = bar
display.visualizations.charting.legend.placement = none

search = eventtype=css-wsa-squid src_ip="*" usage="Personal" | top src_ip | rename src_ip as "Source IP"
dispatch.lookups = 1
dispatch.earliest_time = -60m
dispatch.latest_time = now
is_visible = false
display.general.enablePreview = true
display.general.type = visualizations
display.general.timeRangePicker.show = true
display.visualizations.show = true
display.visualizations.type = charting
display.visualizations.chartHeight = 600px
display.visualizations.charting.chart = bar
display.visualizations.charting.legend.placement = none

search = eventtype=css-wsa-squid cs_username="*" usage="Personal" | top cs_username | rename cs_username as Username
dispatch.lookups = 1
dispatch.earliest_time = -60m
dispatch.latest_time = now
is_visible = false
display.general.enablePreview = true
display.general.type = visualizations
display.general.timeRangePicker.show = true
display.visualizations.show = true
display.visualizations.type = charting
display.visualizations.chartHeight = 600px
display.visualizations.charting.chart = bar
display.visualizations.charting.legend.placement = none

search = eventtype=css-wsa-squid cs_url_host="*" usage="Personal" | top cs_url_host | rename cs_url_host as "Website"
dispatch.lookups = 1
dispatch.earliest_time = -60m
dispatch.latest_time = now
is_visible = false
display.general.enablePreview = true
display.general.type = visualizations
display.general.timeRangePicker.show = true
display.visualizations.show = true
display.visualizations.type = charting
display.visualizations.chartHeight = 600px
display.visualizations.charting.chart = bar
display.visualizations.charting.legend.placement = none

[Cisco WSA - Destinations - By City]
search = eventtype=css-wsa-squid action=allow | iplocation dest_ip | eval City=if(isnotnull(City),City+","+Country,Country) | top City
dispatch.lookups = 1
dispatch.earliest_time = -60m
dispatch.latest_time = now
is_visible = false
display.general.enablePreview = true
display.general.type = statistics
display.general.timeRangePicker.show = true
display.statistics.rowNumbers = true
display.statistics.wrap = true
display.statistics.overlay = none

[Cisco WSA - Destinations - By Country]
search = eventtype=css-wsa-squid action=allow | iplocation dest_ip | top Country
dispatch.lookups = 1
dispatch.earliest_time = -60m
dispatch.latest_time = now
is_visible = false
display.general.enablePreview = true
display.general.type = statistics
display.general.timeRangePicker.show = true
display.statistics.rowNumbers = true
display.statistics.wrap = true
display.statistics.overlay = none

search = eventtype=css-wsa-squid action=block* | top dest_domain
dispatch.lookups = 1
dispatch.earliest_time = -60m
dispatch.latest_time = now
is_visible = false
display.general.enablePreview = true
display.general.type = statistics
display.general.timeRangePicker.show = true
display.statistics.rowNumbers = true
display.statistics.wrap = true
display.statistics.overlay = none

search = eventtype=css-wsa-squid action=allow* | top dest_domain
dispatch.lookups = 1
dispatch.earliest_time = -60m
dispatch.latest_time = now
is_visible = false
display.general.enablePreview = true
display.general.type = statistics
display.general.timeRangePicker.show = true
display.statistics.rowNumbers = true
display.statistics.wrap = true
display.statistics.overlay = none

search = eventtype=css-wsa-squid action=allow* | eval num=lower(dest_port) | eval protocol="tcp" | lookup networkservice "Transport Protocol" as protocol, "Port Number" as dest_port OUTPUT "Service Name" as service | eval method=if(cs_url_scheme="tunnel", service+" [tunnel]", cs_url_scheme+" [direct]") | top method
dispatch.lookups = 1
dispatch.earliest_time = -60m
dispatch.latest_time = now
is_visible = false
display.general.enablePreview = true
display.general.type = statistics
display.general.timeRangePicker.show = true
display.statistics.rowNumbers = true
display.statistics.wrap = true
display.statistics.overlay = none

[Cisco WSA - Network Resources - Bandwidth Utilization]
search = eventtype=css-wsa-squid action=allow* | eval bytes = ((bytes_in + bytes_out) 1-create-csv-with-header.py billyLocalsavedsearches.conf contents.txt master.conf master-section-collection result.csv searchLocalsavedsearches.conf single-section.conf splunk-conf-extraction splunk-conf-extraction20190906.py splunk-conf-extraction.py splunkEnterpriseSecuritySuiteDefaultsavedsearches.conf uniqueHeaders.txt 10) / (1024 1-create-csv-with-header.py billyLocalsavedsearches.conf contents.txt master.conf master-section-collection result.csv searchLocalsavedsearches.conf single-section.conf splunk-conf-extraction splunk-conf-extraction20190906.py splunk-conf-extraction.py splunkEnterpriseSecuritySuiteDefaultsavedsearches.conf uniqueHeaders.txt 1024) | timechart per_second(mb) as "Bandwidth (MBps)"
dispatch.lookups = 1
dispatch.earliest_time = -60m
dispatch.latest_time = now
is_visible = false
display.general.enablePreview = true
display.general.type = statistics
display.general.timeRangePicker.show = true
display.statistics.rowNumbers = true
display.statistics.wrap = true
display.statistics.overlay = none
auto_summarize = 1
auto_summarize.dispatch.earliest_time = -1d@h

[Cisco WSA - Network Resources - By Usage]
search = eventtype=css-wsa-squid action=allow* | eval mb = ((bytes_in + bytes_out) 1-create-csv-with-header.py billyLocalsavedsearches.conf contents.txt master.conf master-section-collection result.csv searchLocalsavedsearches.conf single-section.conf splunk-conf-extraction splunk-conf-extraction20190906.py splunk-conf-extraction.py splunkEnterpriseSecuritySuiteDefaultsavedsearches.conf uniqueHeaders.txt 10) / (1024 1-create-csv-with-header.py billyLocalsavedsearches.conf contents.txt master.conf master-section-collection result.csv searchLocalsavedsearches.conf single-section.conf splunk-conf-extraction splunk-conf-extraction20190906.py splunk-conf-extraction.py splunkEnterpriseSecuritySuiteDefaultsavedsearches.conf uniqueHeaders.txt 1024) | where isnotnull(usage) | timechart per_second(mb) as "Bandwidth (MBps)" by usage
dispatch.lookups = 1
dispatch.earliest_time = -60m
dispatch.latest_time = now
is_visible = false
display.general.enablePreview = true
display.general.type = statistics
display.general.timeRangePicker.show = true
display.statistics.rowNumbers = true
display.statistics.wrap = true
display.statistics.overlay = none
auto_summarize = 1
auto_summarize.dispatch.earliest_time = -1d@h

search = eventtype=css-wsa-squid action=allow* | eval bw = (bytes_in + bytes_out) / (1024 1-create-csv-with-header.py billyLocalsavedsearches.conf contents.txt master.conf master-section-collection result.csv searchLocalsavedsearches.conf single-section.conf splunk-conf-extraction splunk-conf-extraction20190906.py splunk-conf-extraction.py splunkEnterpriseSecuritySuiteDefaultsavedsearches.conf uniqueHeaders.txt 1024) | stats sum(bw) as x by x_webcat_code_full | sort -x | head 10 | rename x as "Transfer Size (MB)", x_webcat_code_full as "Category"
dispatch.lookups = 1
dispatch.earliest_time = -60m
dispatch.latest_time = now
is_visible = false
display.general.enablePreview = true
display.general.type = statistics
display.general.timeRangePicker.show = true
display.statistics.rowNumbers = true
display.statistics.wrap = true
display.statistics.overlay = none

search = eventtype=css-wsa-squid action=allow* | eval bytes = (bytes_in + bytes_out) / (1024 1-create-csv-with-header.py billyLocalsavedsearches.conf contents.txt master.conf master-section-collection result.csv searchLocalsavedsearches.conf single-section.conf splunk-conf-extraction splunk-conf-extraction20190906.py splunk-conf-extraction.py splunkEnterpriseSecuritySuiteDefaultsavedsearches.conf uniqueHeaders.txt 1024) | stats sum(bytes) as x by dest_domain | sort -x | head 10 | rename x as "Transfer Size (MB)", dest_domain as "Destination"
dispatch.lookups = 1
dispatch.earliest_time = -60m
dispatch.latest_time = now
is_visible = false
display.general.enablePreview = true
display.general.type = statistics
display.general.timeRangePicker.show = true
display.statistics.rowNumbers = true
display.statistics.wrap = true
display.statistics.overlay = none

[Cisco WSA - Overview - Requests by Application]
search = eventtype=css-wsa-squid x_avc_app="*" x_avc_app!="-" x_avc_app!="Unknown" | top x_avc_app
dispatch.lookups = 1
dispatch.earliest_time = -60m
dispatch.latest_time = now
is_visible = false
display.general.enablePreview = true
display.general.type = visualizations
display.general.timeRangePicker.show = true
display.visualizations.show = true
display.visualizations.type = charting
display.visualizations.chartHeight = 600px
display.visualizations.charting.chart = pie
display.visualizations.charting.legend.placement = right

[Cisco WSA - Overview - Requests by Category]
search = eventtype=css-wsa-squid | eval Category=if(match(x_webcat_code_abbr,"C_.*"),"[CUSTOM] "+x_webcat_code_full,x_webcat_code_full) | top Category
dispatch.lookups = 1
dispatch.earliest_time = -60m
dispatch.latest_time = now
is_visible = false
display.general.enablePreview = true
display.general.type = visualizations
display.general.timeRangePicker.show = true
display.visualizations.show = true
display.visualizations.type = charting
display.visualizations.chartHeight = 600px
display.visualizations.charting.chart = pie
display.visualizations.charting.legend.placement = right

search = eventtype=css-wsa-squid | eval bytes=bytes_in+bytes_out | stats sum(bytes) as bytecount,count by dest_domain | sort -bytecount | `resize-bytes("bytecount", bytecount)` | rename count as "# Requests", bytecount as "Transfer Size", dest_domain as "Destination"
dispatch.lookups = 1
dispatch.earliest_time = -60m
dispatch.latest_time = now
is_visible = false
display.general.enablePreview = true
display.general.type = statistics
display.general.timeRangePicker.show = true
display.statistics.rowNumbers = true
display.statistics.wrap = true
display.statistics.overlay = heatmap
auto_summarize = 1
auto_summarize.dispatch.earliest_time = -1d@h

search = eventtype=css-wsa-squid | eval action=if(http_result="TCP_DENIED/407","block",action) | where action!="error" | eval cs_username=if(isnull(cs_username) OR cs_username="-","["+c_ip+"]",cs_username) | chart count by cs_username,action | eval count=block+allow | eval f_username=if(match(cs_username,"^["),"*",cs_username) | eval f_ip=if(match(cs_username,"^["),replace(cs_username,"[[]]",""),"*") | sort - count | table cs_username,f_username,f_ip,count,allow,block | rename cs_username as "Username",count as "# Requests", block as "Blocked", allow as "Allowed"
dispatch.lookups = 1
dispatch.earliest_time = -60m
dispatch.latest_time = now
is_visible = false
display.general.enablePreview = true
display.general.type = statistics
display.general.timeRangePicker.show = true
display.statistics.rowNumbers = true
display.statistics.wrap = true
display.statistics.overlay = none
auto_summarize = 1
auto_summarize.dispatch.earliest_time = -1d@h

search = eventtype=css-wsa-squid | eval action=if(http_result="TCP_DENIED/407", "Auth Failed", action) | timechart usenull=f count by action
dispatch.lookups = 1
dispatch.earliest_time = -60m
dispatch.latest_time = now
is_visible = false
display.general.enablePreview = true
display.general.type = visualizations
display.general.timeRangePicker.show = true
display.visualizations.show = true
display.visualizations.type = charting
display.visualizations.chartHeight = 600px
display.visualizations.charting.chart = area
display.visualizations.charting.chart.stackMode = stacked
display.visualizations.charting.chart.nullValueMode = zero
display.visualizations.charting.legend.placement = right
display.visualizations.charting.axisTitleY.text = "# Transactions"
display.visualizations.charting.axisTitleY.visibility = visible
auto_summarize = 1
auto_summarize.dispatch.earliest_time = -1d@h

search = eventtype=css-wsa-squid threat_reason="*" threat_reason!="-" | eval threat_reason=if(http_result="TCP_DENIED/407","Auth Failure",threat_reason) | top threat_reason
dispatch.lookups = 1
dispatch.earliest_time = -60m
dispatch.latest_time = now
is_visible = false
display.general.enablePreview = true
display.general.type = visualizations
display.general.timeRangePicker.show = true
display.visualizations.show = true
display.visualizations.type = charting
display.visualizations.chartHeight = 600px
display.visualizations.charting.chart = pie
display.visualizations.charting.legend.placement = right

[Cisco WSA - Proxy Performance - Bandwidth]
search = eventtype=css-wsa-squid | eval mbytes_in = bytes_in/(1024*1024) | eval mbytes_out = bytes_out/(1024*1024) | timechart sum(mbytes_in) as "Inbound (MB)", sum(mbytes_Out) as "Outbound (MB)"
dispatch.lookups = 1
dispatch.earliest_time = -60m
dispatch.latest_time = now
is_visible = false
display.general.enablePreview = true
display.general.type = statistics
display.general.timeRangePicker.show = true
display.statistics.rowNumbers = true
display.statistics.wrap = true
display.statistics.overlay = none
auto_summarize = 1
auto_summarize.dispatch.earliest_time = -1d@h

[Cisco WSA - Proxy Performance - By Access Policy]
search = eventtype=css-wsa-squid | stats count by access_policy | sort -count
dispatch.lookups = 1
dispatch.earliest_time = -60m
dispatch.latest_time = now
is_visible = false
display.general.enablePreview = true
display.general.type = statistics
display.general.timeRangePicker.show = true
display.statistics.rowNumbers = true
display.statistics.wrap = true
display.statistics.overlay = none
auto_summarize = 1
auto_summarize.dispatch.earliest_time = -1d@h

[Cisco WSA - Proxy Performance - Cache Performance]
search = eventtype=css-wsa-squid cache!="-" | timechart count by cache | rename hit as "Cache Hit", miss as "Cache Miss"
dispatch.lookups = 1
dispatch.earliest_time = -60m
dispatch.latest_time = now
is_visible = false
display.general.enablePreview = true
display.general.type = statistics
display.general.timeRangePicker.show = true
display.statistics.rowNumbers = true
display.statistics.wrap = true
display.statistics.overlay = none
auto_summarize = 1
auto_summarize.dispatch.earliest_time = -1d@h

[Cisco WSA - Proxy Performance - Request Duration]
search = eventtype=css-wsa-squid | timechart avg(duration) as "Avg Duration (ms)", max(duration) as "Peak Duration (ms)"
dispatch.lookups = 1
dispatch.earliest_time = -60m
dispatch.latest_time = now
is_visible = false
display.general.enablePreview = true
display.general.type = statistics
display.general.timeRangePicker.show = true
display.statistics.rowNumbers = true
display.statistics.wrap = true
display.statistics.overlay = none
auto_summarize = 1
auto_summarize.dispatch.earliest_time = -1d@h

search = eventtype=css-wsa-squid | eval count=1 | timechart per_second(count) AS "Requests/sec" by host
dispatch.lookups = 1
dispatch.earliest_time = -60m
dispatch.latest_time = now
is_visible = false
display.general.enablePreview = true
display.general.type = statistics
display.general.timeRangePicker.show = true
display.statistics.rowNumbers = true
display.statistics.wrap = true
display.statistics.overlay = none
auto_summarize = 1
auto_summarize.dispatch.earliest_time = -1d@h

[Cisco WSA - Proxy Performance - SSL Traffic Load]
search = eventtype=css-wsa-squid | eval num=lower(dest_port) | eval protocol="tcp" | lookup networkservice "Transport Protocol" as protocol, "Port Number" as dest_port OUTPUT "Service Name" as service | eval aa = " [" + action + "]" | eval method=if(cs_url_scheme="tunnel",service+aa,cs_url_scheme+aa) | stats count by method | sort -count
dispatch.lookups = 1
dispatch.earliest_time = -60m
dispatch.latest_time = now
is_visible = false
display.general.enablePreview = true
display.general.type = statistics
display.general.timeRangePicker.show = true
display.statistics.rowNumbers = true
display.statistics.wrap = true
display.statistics.overlay = none
auto_summarize = 1
auto_summarize.dispatch.earliest_time = -1d@h

search = eventtype=css-wsa-squid action="allow" cs_url_stem="*search*" | rex field=cs_url "[?&#](?<qarg>(video|query|search|q|search_query|p))(=|/)(?<qstr>[^?&]+)" max_match=0 | rex field=qstr "(?<search_term>w+)" max_match=0 | mvexpand search_term | stats dc(cs_username) as usercount,dc(search_term) as stcount by dest_domain | sort -usercount | rename usercount as "# Unique Users", stcount as "# Search Terms",dest_domain as "Search Provider"
dispatch.lookups = 1
dispatch.earliest_time = -60m
dispatch.latest_time = now
is_visible = false
display.general.enablePreview = true
display.general.type = visualizations
display.general.timeRangePicker.show = true
display.visualizations.show = true
display.visualizations.type = charting
display.visualizations.charting.chart = pie
auto_summarize = 1
auto_summarize.dispatch.earliest_time = -1d@h

search = eventtype=css-wsa-squid action="allow" cs_url_stem="*search*" | rex field=cs_url "[?&#](?<qarg>(video|query|search|q|search_query|p))(=|/)(?<qstr>[^?&]+)" max_match=0 | rex field=qstr "(?<search_term>w+)" max_match=0 | top useother=f search_term | rename count as "# Searches",search_term as "Search Term"
dispatch.lookups = 1
dispatch.earliest_time = -60m
dispatch.latest_time = now
is_visible = false
display.general.enablePreview = true
display.general.type = visualizations
display.general.timeRangePicker.show = true
display.visualizations.show = true
display.visualizations.type = charting
display.visualizations.charting.chart = pie

search = eventtype=css-wsa-squid action="allow" cs_url_stem="*search*" | rex field=cs_url "[?&#](?<qarg>(video|query|search|q|search_query|p))(=|/)(?<qstr>[^?&]+)" max_match=0 | rex field=qstr "(?<search_term>w+)" max_match=0 | top cs_username | rename count as "# Searches",cs_username as "Username"
dispatch.lookups = 1
dispatch.earliest_time = -60m
dispatch.latest_time = now
is_visible = false
display.general.enablePreview = true
display.general.type = visualizations
display.general.timeRangePicker.show = true
display.visualizations.show = true
display.visualizations.type = charting
display.visualizations.charting.chart = pie

search = eventtype=css-wsa-squid http_result!="TCP_DENIED/407" x_wbrs_score!="-" x_wbrs_score<=6.0 | eval Category=if(match(x_webcat_code_abbr,"C_.*"),"[CUSTOM] "+x_webcat_code_full,x_webcat_code_full) | top Category
dispatch.lookups = 1
dispatch.earliest_time = -60m
dispatch.latest_time = now
is_visible = false
display.general.enablePreview = true
display.general.type = statistics
display.general.timeRangePicker.show = true
display.statistics.rowNumbers = true
display.statistics.wrap = true
display.statistics.overlay = none

search = eventtype=css-wsa-squid http_result!="TCP_DENIED/407" x_wbrs_score!="-" x_wbrs_score<=6.0 | top limit=10 src_ip | eval c_ip=src_ip | rename src_ip AS "Client IP"
dispatch.lookups = 1
dispatch.earliest_time = -60m
dispatch.latest_time = now
is_visible = false
display.general.enablePreview = true
display.general.type = statistics
display.general.timeRangePicker.show = true
display.statistics.rowNumbers = true
display.statistics.wrap = true
display.statistics.overlay = none

search = eventtype=css-wsa-squid http_result!="TCP_DENIED/407" x_wbrs_score!="-" x_wbrs_score<=6.0 | top limit=10 dest_domain | rename dest_domain as "Destination"
dispatch.lookups = 1
dispatch.earliest_time = -60m
dispatch.latest_time = now
is_visible = false
display.general.enablePreview = true
display.general.type = statistics
display.general.timeRangePicker.show = true
display.statistics.rowNumbers = true
display.statistics.wrap = true
display.statistics.overlay = none

search = eventtype=css-wsa-squid http_result!="TCP_DENIED/407" X-ScanVerdict=1 | top limit=10 src_ip | eval c_ip=src_ip | rename src_ip as "Client IP"
dispatch.lookups = 1
dispatch.earliest_time = -60m
dispatch.latest_time = now
is_visible = false
display.general.enablePreview = true
display.general.type = statistics
display.general.timeRangePicker.show = true
display.statistics.rowNumbers = true
display.statistics.wrap = true
display.statistics.overlay = none

search = eventtype=css-wsa-squid http_result!="TCP_DENIED/407" X-ScanVerdict=1 | top limit=10 dest_domain | rename dest_domain as "Destination"
dispatch.lookups = 1
dispatch.earliest_time = -60m
dispatch.latest_time = now
is_visible = false
display.general.enablePreview = true
display.general.type = statistics
display.general.timeRangePicker.show = true
display.statistics.rowNumbers = true
display.statistics.wrap = true
display.statistics.overlay = none

search = eventtype=css-wsa-squid http_result!="TCP_DENIED/407" X-ScanVerdict=1 | top limit=10 X-ThreatName | rename X-ThreatName AS "Threat Name"
dispatch.lookups = 1
dispatch.earliest_time = -60m
dispatch.latest_time = now
is_visible = false
display.general.enablePreview = true
display.general.type = statistics
display.general.timeRangePicker.show = true
display.statistics.rowNumbers = true
display.statistics.wrap = true
display.statistics.overlay = none

[Cisco WSA - Security - Traffic Severity Timechart]
search = eventtype=css-wsa-squid http_result!="TCP_DENIED/407" | eval severity=`cisco-wsa-score(x_wbrs_score)` | eval severity=if(X-ScanVerdict=1,"red",severity) | timechart count by severity | table _time,red,orange,yellow,blue,green
dispatch.lookups = 1
dispatch.earliest_time = -60m
dispatch.latest_time = now
is_visible = false
display.general.enablePreview = true
display.general.type = visualizations
display.general.timeRangePicker.show = true
display.visualizations.show = true
display.visualizations.type = charting
display.visualizations.chartHeight = 600px
display.visualizations.charting.chart = area
display.visualizations.charting.chart.stackMode = stacked
display.visualizations.charting.chart.nullValueMode = zero
display.visualizations.charting.chart.style = shiny
display.visualizations.charting.legend.placement = right
display.visualizations.charting.legend.labelStyle.overflowMode = ellipsisEnd
display.visualizations.charting.axisTitleY.text = "# Requests"
display.visualizations.charting.axisTitleY.visibility = visible
auto_summarize = 1
auto_summarize.dispatch.earliest_time = -1d@h

[Cisco WSA - Security - Web Requests by Reputation]
search = eventtype=css-wsa-squid | eval x_wbrs_score=case(x_wbrs_score="ns","unknown",x_wbrs_score="-","unknown",1=1,x_wbrs_score) | eval score=if(x_wbrs_score="unknown","unknown",floor(x_wbrs_score)) | stats count by score | sort score
dispatch.lookups = 1
dispatch.earliest_time = -60m
dispatch.latest_time = now
is_visible = false
display.general.enablePreview = true
display.general.type = visualizations
display.general.timeRangePicker.show = true
display.visualizations.show = true
display.visualizations.type = charting
display.visualizations.chartHeight = 600px
display.visualizations.charting.chart = column
display.visualizations.charting.chart.nullValueMode = gaps
display.visualizations.charting.legend.placement = none
display.visualizations.charting.axisTitleY.text = "# Transactions"
display.visualizations.charting.axisTitleY.visibility = visible
auto_summarize = 1
auto_summarize.dispatch.earliest_time = -1d@h

[Cisco WSA - Web Request Metrics - Average Request Size]
search = eventtype=css-wsa-squid action="allow" | eval bytes=bytes_in + bytes_out | timechart min(bytes) as min, avg(bytes) as avg, max(bytes) as max
dispatch.lookups = 1
dispatch.earliest_time = -60m
dispatch.latest_time = now
is_visible = false
display.general.enablePreview = true
display.general.type = visualizations
display.general.timeRangePicker.show = true
display.visualizations.show = true
display.visualizations.type = charting
display.visualizations.charting.chart = pie
auto_summarize = 1
auto_summarize.dispatch.earliest_time = -1d@h

[Cisco WSA - Web Request Metrics - Cache Efficiency]
search = eventtype=css-wsa-squid action="allow" | stats count by cache
dispatch.lookups = 1
dispatch.earliest_time = -60m
dispatch.latest_time = now
is_visible = false
display.general.enablePreview = true
display.general.type = visualizations
display.general.timeRangePicker.show = true
display.visualizations.show = true
display.visualizations.type = charting
display.visualizations.charting.chart = pie
auto_summarize = 1
auto_summarize.dispatch.earliest_time = -1d@h

search = eventtype=css-wsa-squid cs_username!="-" | timechart dc(cs_username) as "# Unique Users"
dispatch.lookups = 1
dispatch.earliest_time = -60m
dispatch.latest_time = now
is_visible = false
display.general.enablePreview = true
display.general.type = visualizations
display.general.timeRangePicker.show = true
display.visualizations.show = true
display.visualizations.type = charting
display.visualizations.charting.chart = pie
auto_summarize = 1
auto_summarize.dispatch.earliest_time = -1d@h

search = eventtype=css-wsa-squid cs_username!="-" | stats dc(src_ip) as ipcount by cs_username | stats count by ipcount
dispatch.lookups = 1
dispatch.earliest_time = -60m
dispatch.latest_time = now
is_visible = false
display.general.enablePreview = true
display.general.type = visualizations
display.general.timeRangePicker.show = true
display.visualizations.show = true
display.visualizations.type = charting
display.visualizations.charting.chart = pie
auto_summarize = 1
auto_summarize.dispatch.earliest_time = -1d@h

search = eventtype=css-wsa-squid cs_username!="-" | stats dc(cs_user_agent) as uacount by cs_username | stats count by uacount
dispatch.lookups = 1
dispatch.earliest_time = -60m
dispatch.latest_time = now
is_visible = false
display.general.enablePreview = true
display.general.type = visualizations
display.general.timeRangePicker.show = true
display.visualizations.show = true
display.visualizations.type = charting
display.visualizations.charting.chart = pie
auto_summarize = 1
auto_summarize.dispatch.earliest_time = -1d@h

###### Lookup Generating Searches ######

## CIM - Vendor Product Tracker - Lookup Gen Breakdown
## 1 - get vendor_product values from Malware data model
## 2 - field renaming
## 3 - set model="Malware"
## 4 - get vendor_product values from Network_Traffic data model
## 5 - field renaming
## 6 - set model="Network_Traffic"
## 7 - get vendor_product values from Intrusion_Detection data model
## 8 - field renaming
## 9 - set model="Intrusion_Detection"
## 10 - get vendor_product values from Vulnerability namespace
## 11 - field renaming
## 12 - set model="Vulnerabilities"
## 13 - consolidate vendor_product values
## 14 - input existing values
## 15 - consolidate vendor_product values for the second time
## 16 - write lookup
## 17 - purge results
[CIM - Vendor Product Tracker - Lookup Gen]
action.email.sendresults = 0
cron_schedule = 5,20,35,50 1-create-csv-with-header.py billyLocalsavedsearches.conf contents.txt master.conf master-section-collection result.csv searchLocalsavedsearches.conf single-section.conf splunk-conf-extraction splunk-conf-extraction20190906.py splunk-conf-extraction.py splunkEnterpriseSecuritySuiteDefaultsavedsearches.conf uniqueHeaders.txt 1-create-csv-with-header.py billyLocalsavedsearches.conf contents.txt master.conf master-section-collection result.csv searchLocalsavedsearches.conf single-section.conf splunk-conf-extraction splunk-conf-extraction20190906.py splunk-conf-extraction.py splunkEnterpriseSecuritySuiteDefaultsavedsearches.conf uniqueHeaders.txt 1-create-csv-with-header.py billyLocalsavedsearches.conf contents.txt master.conf master-section-collection result.csv searchLocalsavedsearches.conf single-section.conf splunk-conf-extraction splunk-conf-extraction20190906.py splunk-conf-extraction.py splunkEnterpriseSecuritySuiteDefaultsavedsearches.conf uniqueHeaders.txt 1-create-csv-with-header.py billyLocalsavedsearches.conf contents.txt master.conf master-section-collection result.csv searchLocalsavedsearches.conf single-section.conf splunk-conf-extraction splunk-conf-extraction20190906.py splunk-conf-extraction.py splunkEnterpriseSecuritySuiteDefaultsavedsearches.conf uniqueHeaders.txt
description = Maintains a list of vendor_product values and the first and list time they have been seen
dispatch.earliest_time = -30m@m
dispatch.latest_time = +0s
enableSched = 0
is_visible = false
search = | tstats prestats=true summariesonly=true min(_time),max(_time) from datamodel=Malware.Malware_Attacks by Malware_Attacks.vendor_product | `drop_dm_object_name("Malware_Attacks")` | eval model="Malware" | tstats prestats=true summariesonly=true append=true min(_time),max(_time) from datamodel=Network_Traffic.All_Traffic by All_Traffic.vendor_product | `drop_dm_object_name("All_Traffic")` | eval model=if(isnull(model),"Network_Traffic",model) | tstats prestats=true summariesonly=true append=true min(_time),max(_time) from datamodel=Intrusion_Detection.IDS_Attacks by IDS_Attacks.vendor_product | `drop_dm_object_name("IDS_Attacks")` | eval model=if(isnull(model),"Intrusion_Detection",model) | tstats prestats=true summariesonly=true append=true min(_time),max(_time) from datamodel=Vulnerabilities.Vulnerabilities by Vulnerabilities.vendor_product | `drop_dm_object_name("Vulnerabilities")` | eval model=if(isnull(model),"Vulnerabilities",model) | stats min(_time) as firstTime,max(_time) as lastTime by vendor_product,model | inputlookup append=true cim_vendor_product_tracker | stats min(firstTime) as firstTime,max(lastTime) as lastTime by vendor_product,model | outputlookup cim_vendor_product_tracker | stats count


###### Report Searches ######
description = Maintains a data cube of DMA statistics for use in Datamodel Audit view
dispatchAs = user
dispatch.latest_time = now
is_visible = false
search = | `datamodel("Splunk_Audit", "Datamodel_Acceleration")` | `drop_dm_object_name("Datamodel_Acceleration")` | join type=outer last_sid [| rest splunk_server=local count=0 /services/search/jobs reportSearch=summarize* | rename sid as last_sid | fields last_sid,runDuration] | eval "size(MB)"=round(size/1048576,1), "retention(days)"=if(retention==0,"unlimited",round(retention/86400,1)), "complete(%)"=round(complete*100,1), "runDuration(s)"=round(runDuration,1)

[CIM - Top Data Model Accelerations By Size]
action.email.reportServerEnabled = 0
alert.track = 0
dispatch.latest_time = now
display.general.enablePreview = 1
display.general.timeRangePicker.show = false
display.general.type = visualizations
display.statistics.rowNumbers = 0
display.statistics.wrap = 0
display.visualizations.charting.chart = bar
display.visualizations.charting.drilldown = all
display.visualizations.chartHeight = 350
display.visualizations.show = 1
search = | `datamodel("Splunk_Audit", "Datamodel_Acceleration")` | `drop_dm_object_name("Datamodel_Acceleration")` | eval size(MB)=size/1048576 | sort 100 - size | table datamodel,size(MB)

[CIM - Top Data Model Accelerations By Run Duration]
action.email.reportServerEnabled = 0
alert.track = 0
dispatch.latest_time = now
display.general.enablePreview = 1
display.general.timeRangePicker.show = false
display.general.type = visualizations
display.statistics.rowNumbers = 0
display.statistics.wrap = 0
display.visualizations.charting.chart = bar
display.visualizations.charting.drilldown = all
display.visualizations.chartHeight = 350
display.visualizations.show = 1
search = | `datamodel("Splunk_Audit", "Datamodel_Acceleration")` | `drop_dm_object_name("Datamodel_Acceleration")` | join type=outer last_sid [| rest splunk_server=local count=0 /services/search/jobs reportSearch=summarize* | rename sid as last_sid | fields last_sid,runDuration] | sort 100 - runDuration | table datamodel,runDuration

action.email.reportServerEnabled = 0
alert.track = 0
dispatch.latest_time = now
display.general.enablePreview = 1
display.general.timeRangePicker.show = false
display.general.type = statistics
display.statistics.drilldown = row
display.statistics.rowNumbers = 0
display.statistics.wrap = 0
display.visualizations.show = 0
search = | `datamodel("Splunk_Audit", "Datamodel_Acceleration")` | `drop_dm_object_name("Datamodel_Acceleration")` | eval size(MB)=round(size/1048576,1) | eval retention(days)=retention/86400 | eval complete(%)=round(complete*100,1) | sort 100 + datamodel | fieldformat earliest=strftime(earliest, "%m/%d/%Y %H:%M:%S") | fieldformat latest=strftime(latest, "%m/%d/%Y %H:%M:%S") | fields datamodel,app,cron,retention(days),earliest,latest,is_inprogress,complete(%),size(MB),last_error
